# Universal Topic Research Agent

Automated topic discovery and research system for SaaS companies. Finds trending topics in your industry before competitors do, validates SEO opportunities, and generates professional research reports with real citations.

**Business Value**: Stop guessing what to write about. Let AI discover high-demand topics with low competition, then deliver ready-to-use research reports.

## Features

- **Automated Topic Discovery** - Monitor 100+ RSS feeds, Reddit discussions, Google Trends, and autocomplete suggestions
- **Intelligent Feed Discovery** - Find industry-relevant feeds automatically using OPML seeds + Gemini expansion + SerpAPI
- **5-Stage Content Pipeline** - Transform raw topics into prioritized, researched content opportunities
- **Professional Research Reports** - 5-6 page sourced reports at $0.02/topic using qwen + Tavily API
- **Dual Research Backend** - Primary: gpt-researcher (with citations), Fallback: Gemini CLI (faster, no citations)
- **Cost-Optimized** - Topic discovery FREE, research $0.02/topic (qwen-2.5-32b-instruct via OpenRouter)
- **Notion Integration** - Review and approve discovered topics in Notion before writing
- **Smart Deduplication** - MinHash/LSH similarity detection keeps duplicate rate <5%
- **Background Automation** - Daily topic collection (2 AM) and weekly Notion sync (Monday 9 AM)

## Business Logic - Why Each Component Exists

### Topic Discovery (FREE - No API costs)

#### **FeedDiscoveryAgent** - Auto-discover industry RSS feeds
**WHY**: Manually finding 100+ relevant feeds is tedious. Automate it.

**HOW**:
- Stage 1: OPML seeds + Gemini CLI expansion (7 base feeds â†’ 20-30 feeds)
- Stage 2: SerpAPI search + feedfinder2 auto-detection (10 domains â†’ 10-30 feeds)
- Circuit breaker: 3 req/day SerpAPI limit (safety margin on 100/month free tier)

**OUTPUT**: 30-50 RSS feeds covering your entire industry

#### **RSSCollector** - Monitor blogs and news sites
**WHY**: Discover trending topics from thought leaders before they hit mainstream.

**WHAT**:
- Monitors 100+ industry blogs/news sites
- Conditional GET (ETag/Last-Modified) for bandwidth optimization
- Full content extraction via trafilatura (handles summary-only feeds)
- Feed health tracking (skip failing feeds after 5 consecutive failures)

**OUTPUT**: 50-100 new articles/week per market configuration

#### **RedditCollector** - Discover what your audience is asking
**WHY**: Reddit shows unfiltered customer questions and pain points.

**WHAT**:
- Multiple sorting methods (hot, new, top, rising)
- Comment extraction (configurable depth)
- Quality filtering (min score, engagement, content length)
- Subreddit health tracking

**OUTPUT**: 20-50 high-engagement discussions/week

#### **TrendsCollector** - Catch rising topics early (Gemini CLI backend)
**WHY**: Catch topics while they're trending, before saturation.

**WHAT**:
- Migrated from pytrends (DEAD, archived April 2025) to Gemini CLI
- Trending searches (daily/realtime by region: DE, US, FR, etc.)
- Related queries (top/rising for keywords)
- Interest over time (search volume trends)

**OUTPUT**: 10-20 trending topics/day per market, 100% FREE & UNLIMITED

**NOTE**: pytrends was replaced with Gemini CLI in November 2025 due to Google 404/429 errors and maintainer abandonment. Gemini CLI has no rate limits and uses real-time web search data.

#### **AutocompleteCollector** - Find high-intent search queries
**WHY**: Autocomplete shows what people are actively searching for RIGHT NOW.

**WHAT**:
- 3 expansion types: Alphabet (a-z), Questions (what/how/why), Prepositions (for/with/vs)
- Google autocomplete API integration
- Smart caching (30-day TTL)
- Language support (de, en, fr, etc.)

**OUTPUT**: 50-200 autocomplete suggestions per seed keyword

### 5-Stage ContentPipeline (THE CORE)

**Purpose**: Transform discovered topics into actionable, prioritized content opportunities.

**Architecture**: Each stage builds on the previous, enriching the topic with research, keywords, and strategic insights.

---

#### **Stage 1: Competitor Research** (CompetitorResearchAgent)
**WHY**: Don't write content your competitors already dominate. Find gaps where you can win.

**WHAT**:
- Analyze 5 competitors in your niche
- Identify content gaps (topics they ignore or under-serve)
- Spot trending topics in your industry
- Provide strategic recommendations

**COST**: FREE (Gemini CLI)

**OUTPUT**: "Write about GDPR for German SMBs" (gap) vs "Generic cloud intro" (saturated)

---

#### **Stage 2: Keyword Research** (KeywordResearchAgent)
**WHY**: Ensure topics have search demand and aren't too competitive.

**WHAT**:
- Find 1 primary keyword (best fit for topic)
- Find 10 secondary keywords (semantic variations)
- Generate 3-5 long-tail keywords (specific phrases, 3-5 words)
- Include "People also ask" questions
- Provide search volume, competition, difficulty scores

**COST**: FREE (Gemini CLI)

**OUTPUT**: Primary: "Cloud Computing fÃ¼r KMU", Long-tail: "Cloud-Migration GDPR-konform", Difficulty: 42/100

---

#### **Stage 3: Deep Research** (DeepResearcher + gpt-researcher)
**WHY**: Create authoritative content with real citations, not generic AI fluff.

**WHAT**:
- Generate 5-6 page professional reports (1500-2500 words)
- Find 10-20 real web sources (not hallucinated)
- Extract citations from authoritative sites (industry reports, studies, news)
- Context-aware queries (domain, market, language, vertical)
- Integrate competitor gaps and keywords from Stages 1-2

**HOW**:
- **Primary**: gpt-researcher + qwen/qwen-2.5-32b-instruct (via OpenRouter) + Tavily API
- **Fallback**: Gemini CLI (faster, but no citations)

**COST**: $0.02 per topic (qwen $0.14/M input + Tavily API)

**OUTPUT**: Professional research report with 10-20 real web sources (e.g., Grand View Research, EY reports, Fortune Business Insights)

**ENABLED**: Stage 3 is ENABLED by default (`enable_deep_research=True`). Gemini CLI is only a fallback.

---

#### **Stage 4: Content Optimization**
**WHY**: Combine all insights (gaps, keywords, research) into one enriched topic.

**WHAT**:
- Add competitor gaps as topic description
- Attach research report with full text
- Extract source URLs as citations
- Merge keywords from Stage 2
- Calculate word count

**OUTPUT**: Topic object with research report, citations, keywords, description, word count

---

#### **Stage 5: Scoring & Ranking**
**WHY**: Prioritize which topics to write about first (demand vs opportunity).

**WHAT**: Calculate 5 scores (0-1 scale, then converted to 1-10 priority):
- **demand_score**: Search volume + engagement (35% weight)
- **opportunity_score**: Low competition + content gaps (30% weight)
- **fit_score**: Domain/market/vertical alignment (20% weight)
- **novelty_score**: Trending + uniqueness (15% weight)
- **priority_score**: Weighted combination of all factors

**Formula**: `priority = (0.35Ã—demand) + (0.30Ã—opportunity) + (0.20Ã—fit) + (0.15Ã—novelty)`

**OUTPUT**: Priority-ranked topics (1-10 scale, 10 = highest)

---

### Processing & Storage

#### **Deduplicator** - Keep duplicate rate <5%
**WHY**: Don't waste time on duplicate topics from multiple sources.

**WHAT**:
- MinHash/LSH similarity detection
- Canonical URL normalization
- <5% duplicate rate target

#### **Topic Clustering** - Group related topics
**WHY**: Identify topic themes and content series opportunities.

**WHAT**:
- TF-IDF vectorization
- HDBSCAN clustering (auto-determines K)
- LLM-based cluster labeling
- Noise handling

#### **Entity Extractor** - Extract entities and keywords
**WHY**: Tag topics for filtering and organization.

**WHAT**:
- LLM-based NER (qwen-turbo via OpenRouter)
- Batch processing with skip_errors support
- Statistics tracking
- 30-day caching

#### **Notion Topics Sync** - Editorial review interface
**WHY**: Human review of discovered topics before committing to writing.

**WHAT**:
- Sync Topic objects to Notion database
- Rate-limited (2.5 req/sec)
- Batch processing
- Update existing pages or create new

---

## Architecture Highlights

### Cost Structure (Transparent Pricing)

| Component | Method | Cost |
|-----------|--------|------|
| Feed Discovery | OPML + Gemini CLI + SerpAPI | FREE (3 req/day limit) |
| RSS Collection | feedparser + trafilatura | FREE |
| Reddit Collection | PRAW API | FREE |
| Trends Collection | Gemini CLI | FREE (unlimited) |
| Autocomplete | Google API | FREE |
| **Stage 1** (Competitor Research) | Gemini CLI | FREE |
| **Stage 2** (Keyword Research) | Gemini CLI | FREE |
| **Stage 3** (Deep Research) | qwen + Tavily API | **$0.02/topic** |
| **Stage 4** (Content Optimization) | No API calls | FREE |
| **Stage 5** (Scoring) | No API calls | FREE |
| **Total per researched topic** | | **$0.02** |

**Weekly Cost (50 topics)**: $1.00
**Monthly Cost (200 topics)**: $4.00

### Key Technical Patterns

**Dual Backend for Deep Research**:
- Primary: gpt-researcher + qwen/qwen-2.5-32b-instruct (via OpenRouter) + Tavily API
- Fallback: Gemini CLI (faster, no citations, FREE)
- Auto-fallback on gpt-researcher errors

**Rate Limiting**:
- Tavily API: 5 req/s (documented limit)
- DuckDuckGo: 1 req/s (conservative)
- Notion API: 2.5 req/s (safety margin on 3 req/s official limit)
- Reddit API: 60 req/min (PRAW default)

**Caching Strategy**:
- Feed metadata: 30-day TTL
- Autocomplete suggestions: 30-day TTL
- Trends data: 1h TTL (trending searches), 24h TTL (interest data)
- Deep research: Store results to avoid re-research
- LLM processing: 30-day TTL

**Async Pipeline**:
- All 5 stages run sequentially
- Progress tracking via callbacks
- Statistics tracking per stage
- Graceful degradation on failures

**Data Flow**:
```
Discovery â†’ Collection â†’ Deduplication â†’ Clustering â†’ ContentPipeline â†’ Notion
    â†“           â†“              â†“              â†“              â†“              â†“
  Feeds       RSS         MinHash/LSH    TF-IDF+        5 stages      Editorial
  Reddit      Reddit                     HDBSCAN        (Research,     Review
  Trends                                 LLM Labels     Keywords,
  Autocomplete                                          Deep Research,
                                                        Optimize,
                                                        Score)
```

---

## Current Status

### Development Progress

- âœ… **Week 1: Foundation Complete** (7/7 components, 100%)
  - Central logging system (structlog)
  - Document/Topic models (Pydantic V2)
  - Configuration system (YAML + validation)
  - SQLite schema (documents, topics, research_reports)
  - LLM processor (qwen-turbo, replaces 5GB NLP stack)
  - Deduplicator (MinHash/LSH, <5% duplicate rate)
  - Huey task queue (SQLite backend, DLQ, cron)

- âœ… **Week 2: Core Collectors Complete** (10/10 components, 100%)
  - Feed Discovery (OPML + Gemini + SerpAPI)
  - RSS Collector (feedparser + trafilatura)
  - Reddit Collector (PRAW + comment extraction)
  - Trends Collector (Gemini CLI backend, migrated from pytrends)
  - Autocomplete Collector (Google API, 3 expansion types)
  - Topic Clustering (TF-IDF + HDBSCAN + LLM labeling)
  - Entity Extractor (LLM-based NER)
  - Deep Research Wrapper (gpt-researcher + Gemini CLI fallback)
  - Notion Topics Sync (rate-limited, batch support)
  - 5-Stage ContentPipeline (integrated with UI)

- ðŸ”„ **Current Session 021**: E2E Testing & Pipeline Validation
  - Stage 3 Deep Research ENABLED by default
  - E2E test created (331 lines, 4 test functions)
  - Successfully generated PropTech report with 14 real sources
  - Playwright UI tests added (Topic Research page)
  - Minor async/await error in Stage 3 return path (research works, error is cosmetic)

### Test Metrics

- **Total Tests**: 192 passing (128 collectors + 22 clusterer + 42 foundation)
- **Coverage**: 94.67% overall, 80%+ per component
- **TDD Compliance**: 100% (all tests written first)
- **E2E Integration Tests**: 24 tests (RSS: 13, Reddit: 11)

### Next Steps

- [ ] Complete full pipeline E2E test (Feed Discovery â†’ RSS â†’ Dedup â†’ Clustering â†’ Deep Research â†’ Notion Sync)
- [ ] Test with real PropTech/SaaS topics end-to-end
- [ ] Validate acceptance criteria (50+ topics/week, <5% dedup rate)
- [ ] Fix minor async/await error in Stage 3 return path
- [ ] Extend Topic model with score fields (demand_score, opportunity_score, fit_score, novelty_score, priority_score)

---

## Setup

### Prerequisites

- Python 3.11+
- [Gemini CLI](https://ai.google.dev/gemini-api/docs/) v0.11.3+ (for FREE web research)
- Notion account with integration created
- OpenRouter API key (for qwen models)
- Tavily API key (for web search with citations)
- Reddit API credentials (client_id, client_secret, user_agent)
- SerpAPI key (optional, for feed discovery Stage 2, 100 req/month free tier)

### Installation

1. **Clone and navigate**:
   ```bash
   cd /home/projects/content-creator
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   pip install -r requirements-topic-research.txt
   ```

   **IMPORTANT**: `requirements-topic-research.txt` pins langchain ecosystem to <1.0 due to gpt-researcher 0.14.4 dependency. langchain 1.0+ removed `langchain.docstore` module causing import errors.

3. **Configure environment** (`.env` or `/home/envs/*.env`):
   ```bash
   # OpenRouter (for qwen models)
   OPENROUTER_API_KEY=your_openrouter_key

   # Tavily API (for web search with citations)
   TAVILY_API_KEY=your_tavily_key

   # Notion
   NOTION_TOKEN=your_notion_token
   NOTION_PAGE_ID=your_page_id

   # Reddit API
   REDDIT_CLIENT_ID=your_client_id
   REDDIT_CLIENT_SECRET=your_client_secret
   REDDIT_USER_AGENT=topic_research_agent/1.0

   # SerpAPI (optional, for feed discovery)
   SERPAPI_KEY=your_serpapi_key

   # Settings
   CONTENT_LANGUAGE=de
   NOTION_RATE_LIMIT=2.5
   ```

   **Note**: DeepResearcher auto-loads `OPENROUTER_API_KEY` and `TAVILY_API_KEY` from `/home/envs/openrouter.env` and `/home/envs/tavily.env` if not set in environment.

4. **Install Gemini CLI** (FREE web research):
   ```bash
   npm install -g @google/generative-ai-cli
   gemini --version  # Verify installation
   ```

5. **Create Notion databases**:
   ```bash
   python setup_notion.py
   ```
   Creates 5 databases: Projects, Blog Posts, Social Posts, Research Data, Topics

6. **Launch Streamlit UI**:
   ```bash
   streamlit run streamlit_app.py
   ```
   Navigate to **Topic Research** page to test 5-stage pipeline

7. **Start background task queue** (optional):
   ```bash
   python -m src.tasks.huey_tasks
   ```
   Enables daily topic collection (2 AM) and weekly Notion sync (Monday 9 AM)

### Verify Setup

Test Notion connection:
```bash
python test_notion_connection.py
```

Check databases:
```bash
python check_databases.py
```

Test Gemini CLI:
```bash
gemini chat "What is PropTech?"
```

---

## Usage

### Via Streamlit UI (Topic Research Page)

1. **Configure Market**:
   - Select or create market configuration (domain, market, language, vertical)
   - Examples: PropTech Germany, Fashion France, SaaS USA

2. **Discover Topics**:
   - Enter seed keyword (e.g., "PropTech Trends")
   - Click "Discover Topics" â†’ Runs collectors (RSS, Reddit, Trends, Autocomplete)
   - Review discovered topics (title, source, engagement score)

3. **Research Topic**:
   - Select topic from discovered list
   - Click "Research Topic" â†’ Runs 5-stage ContentPipeline
   - Watch progress with real-time updates:
     ```
     Stage 1/5: Analyzing competitors... âœ“
     Stage 2/5: Researching keywords... âœ“
     Stage 3/5: Generating research report... â³ (60-90s)
     Stage 4/5: Optimizing content... âœ“
     Stage 5/5: Calculating scores... âœ“
     ```

4. **Review in Notion**:
   - Open researched topic in Notion database
   - Read research report with citations
   - Review competitor gaps, keywords, priority score
   - Approve or reject topic for content writing

### Via Python API

```python
from src.agents.content_pipeline import ContentPipeline
from src.agents.competitor_research_agent import CompetitorResearchAgent
from src.agents.keyword_research_agent import KeywordResearchAgent
from src.research.deep_researcher import DeepResearcher
from src.models.topic import Topic
from src.models.config import MarketConfig

# Initialize pipeline
pipeline = ContentPipeline(
    competitor_agent=CompetitorResearchAgent(api_key),
    keyword_agent=KeywordResearchAgent(api_key),
    deep_researcher=DeepResearcher(),
    enable_deep_research=True  # Stage 3 enabled by default
)

# Create topic
topic = Topic(
    title="PropTech Trends 2025",
    source="manual",
    engagement_score=75.0,
    trending_score=80.0
)

# Create market config
config = MarketConfig(
    domain="PropTech",
    market="Germany",
    language="de",
    vertical="Real Estate Technology"
)

# Process topic (5 stages)
def progress_callback(stage: int, message: str):
    print(f"Stage {stage}/5: {message}")

enhanced_topic = await pipeline.process_topic(
    topic=topic,
    config=config,
    progress_callback=progress_callback
)

# Review results
print(f"Priority Score: {enhanced_topic.priority}/10")
print(f"Research Report: {len(enhanced_topic.research_report)} chars")
print(f"Citations: {len(enhanced_topic.citations)} sources")
```

### Background Automation (Huey)

**Daily Topic Collection** (runs at 2 AM):
```python
from src.tasks.huey_tasks import daily_collection

# Triggered automatically via cron
# Collects topics from all configured markets
# Deduplicates and stores in SQLite
```

**Weekly Notion Sync** (runs Monday 9 AM):
```python
from src.tasks.huey_tasks import weekly_notion_sync

# Triggered automatically via cron
# Syncs top 10 topics per market to Notion
# Rate-limited to 2.5 req/sec
```

---

## Important Notes

### Stage 3 Deep Research Configuration

**ENABLED by default**: `enable_deep_research=True` in `ContentPipeline.__init__()`

**Primary method**: gpt-researcher + qwen/qwen-2.5-32b-instruct (via OpenRouter) + Tavily API
- Cost: $0.02 per topic
- Output: 5-6 page reports with 10-20 real citations
- Sources: Industry reports (Grand View Research, Gartner), news articles, academic papers

**Fallback method**: Gemini CLI
- Cost: FREE
- Output: 800-1200 word reports WITHOUT citations
- Trigger: Auto-fallback when gpt-researcher fails

**To disable Stage 3**:
```python
pipeline = ContentPipeline(
    competitor_agent=competitor_agent,
    keyword_agent=keyword_agent,
    deep_researcher=deep_researcher,
    enable_deep_research=False  # Skip deep research
)
```

### Gemini CLI vs gpt-researcher

**When Gemini CLI is used**:
- âŒ gpt-researcher import error (dependency issues)
- âŒ Tavily API quota exceeded
- âŒ OpenRouter API error
- âœ… User explicitly disables Stage 3

**Recommendation**: Keep Stage 3 enabled. gpt-researcher provides significantly better results with real citations. Gemini CLI is a safety net, not the primary method.

### API Key Loading

DeepResearcher auto-loads API keys from `/home/envs/` if not set in environment:
- `OPENROUTER_API_KEY` from `/home/envs/openrouter.env`
- `TAVILY_API_KEY` from `/home/envs/tavily.env`

**For qwen models via OpenRouter**:
- Sets `OPENAI_API_KEY = OPENROUTER_API_KEY`
- Sets `OPENAI_API_BASE = https://openrouter.ai/api/v1`

This allows gpt-researcher to use OpenRouter's OpenAI-compatible API without code changes.

### Topic Model Schema (Phase 2 Enhancement)

**Current fields**:
- `title`, `description`, `source`, `url`, `priority` (1-10)
- `engagement_score`, `trending_score` (0-100)
- `research_report`, `citations`, `word_count`

**Missing fields** (to be added):
- `demand_score`, `opportunity_score`, `fit_score`, `novelty_score`, `priority_score` (0-1 scale)

**Workaround**: Stage 5 converts `priority_score` to `priority` field (1-10 scale) until model is extended.

---

## Troubleshooting

### gpt-researcher Errors

**Error**: `No module named 'langchain.docstore'`

**Cause**: langchain 1.0+ removed `langchain.docstore` module. gpt-researcher 0.14.4 requires langchain<1.0.

**Fix**: Install langchain <1.0:
```bash
pip install 'langchain<1.0' 'langchain-core<1.0' 'langchain-community<1.0' 'langchain-text-splitters<1.0'
```

**Permanent fix**: Use `requirements-topic-research.txt` which pins correct versions.

---

**Error**: `object list can't be used in 'await' expression`

**Cause**: Minor bug in Stage 3 return path (cosmetic error, research works correctly).

**Status**: Known issue, does not affect functionality. Research report and citations are generated successfully.

---

**Error**: `OPENAI_API_KEY not found`

**Cause**: gpt-researcher expects `OPENAI_API_KEY` even when using OpenRouter.

**Fix**: DeepResearcher auto-loads and maps `OPENROUTER_API_KEY â†’ OPENAI_API_KEY`. Ensure `/home/envs/openrouter.env` exists with valid key.

---

### Gemini CLI Errors

**Error**: `gemini: command not found`

**Fix**: Install Gemini CLI:
```bash
npm install -g @google/generative-ai-cli
```

**Verify**: `gemini --version`

---

**Error**: Gemini CLI hangs indefinitely

**Cause**: Passing prompts as positional arguments triggers interactive mode.

**Fix**: Use stdin method (already implemented in all agents):
```python
subprocess.run(['gemini', 'chat'], input=prompt, ...)  # âœ… Correct
subprocess.run(['gemini', 'chat', prompt], ...)        # âŒ Hangs
```

---

### Notion Rate Limits

**Error**: `Rate limit exceeded`

**Fix**: Reduce `NOTION_RATE_LIMIT` in `.env`:
```bash
NOTION_RATE_LIMIT=2.0  # Lower from 2.5
```

**Check sync logs**: `cache/sync_logs/sync_status.json`

**Re-run sync**: Cached topics persist on disk, safe to retry.

---

### SerpAPI Quota

**Warning**: Circuit breaker triggered (3 req/day limit)

**Cause**: Free tier allows 100 req/month. Circuit breaker enforces 3 req/day safety margin.

**Fix**:
- Wait 24 hours for quota reset
- Upgrade SerpAPI plan
- Use Stage 1 only (OPML + Gemini CLI expansion) without Stage 2

---

## Testing

### Run All Tests

```bash
# Unit tests only (fast)
pytest tests/test_unit/ -v

# Integration tests (requires API keys)
pytest tests/test_integration/ -v

# All tests with coverage
pytest --cov=src --cov-report=html

# Specific component
pytest tests/test_unit/test_content_pipeline.py -v
```

### E2E Tests

**ContentPipeline E2E** (costs $0.02-0.05):
```bash
pytest tests/test_integration/test_full_pipeline_e2e.py -v
```

**Playwright UI Tests** (fast):
```bash
pytest tests/test_playwright_ui.py::test_topic_research_page_loads -v
```

**Playwright Full Pipeline** (skipped by default, costs $0.02-0.05):
```bash
pytest tests/test_playwright_ui.py::test_topic_research_full_pipeline -v --run-expensive
```

### TDD Workflow

1. Write test first (RED)
2. Implement minimal code to pass (GREEN)
3. Refactor for quality (REFACTOR)
4. Repeat

**Coverage target**: 80%+ per component, 94.67% overall

---

## Project Structure

```
content-creator/
â”œâ”€â”€ src/                          # Core application
â”‚   â”œâ”€â”€ agents/                   # AI agents
â”‚   â”‚   â”œâ”€â”€ competitor_research_agent.py
â”‚   â”‚   â”œâ”€â”€ keyword_research_agent.py
â”‚   â”‚   â”œâ”€â”€ research_agent.py
â”‚   â”‚   â””â”€â”€ content_pipeline.py   # 5-stage pipeline
â”‚   â”œâ”€â”€ collectors/               # Topic discovery
â”‚   â”‚   â”œâ”€â”€ feed_discovery.py     # OPML + Gemini + SerpAPI
â”‚   â”‚   â”œâ”€â”€ rss_collector.py      # feedparser + trafilatura
â”‚   â”‚   â”œâ”€â”€ reddit_collector.py   # PRAW API
â”‚   â”‚   â”œâ”€â”€ trends_collector.py   # Gemini CLI (migrated from pytrends)
â”‚   â”‚   â””â”€â”€ autocomplete_collector.py  # Google autocomplete
â”‚   â”œâ”€â”€ research/                 # Deep research
â”‚   â”‚   â””â”€â”€ deep_researcher.py    # gpt-researcher wrapper
â”‚   â”œâ”€â”€ processors/               # Data processing
â”‚   â”‚   â”œâ”€â”€ deduplicator.py       # MinHash/LSH
â”‚   â”‚   â”œâ”€â”€ topic_clusterer.py    # TF-IDF + HDBSCAN
â”‚   â”‚   â”œâ”€â”€ entity_extractor.py   # LLM-based NER
â”‚   â”‚   â””â”€â”€ llm_processor.py      # qwen-turbo wrapper
â”‚   â”œâ”€â”€ database/                 # Storage
â”‚   â”‚   â””â”€â”€ sqlite_manager.py     # SQLite + FTS5
â”‚   â”œâ”€â”€ notion_integration/       # Notion sync
â”‚   â”‚   â”œâ”€â”€ notion_client.py      # Rate-limited client
â”‚   â”‚   â””â”€â”€ topics_sync.py        # Topic â†’ Notion sync
â”‚   â”œâ”€â”€ tasks/                    # Background jobs
â”‚   â”‚   â””â”€â”€ huey_tasks.py         # Daily collection, weekly sync
â”‚   â”œâ”€â”€ models/                   # Data models
â”‚   â”‚   â”œâ”€â”€ document.py           # Document (Pydantic)
â”‚   â”‚   â”œâ”€â”€ topic.py              # Topic (Pydantic)
â”‚   â”‚   â””â”€â”€ config.py             # MarketConfig
â”‚   â”œâ”€â”€ ui/                       # Streamlit UI
â”‚   â”‚   â””â”€â”€ pages/
â”‚   â”‚       â””â”€â”€ topic_research.py # Topic Research page
â”‚   â””â”€â”€ utils/                    # Utilities
â”‚       â”œâ”€â”€ logger.py             # structlog
â”‚       â””â”€â”€ config_loader.py      # YAML loader
â”œâ”€â”€ tests/                        # TDD tests
â”‚   â”œâ”€â”€ test_unit/                # Unit tests (fast)
â”‚   â””â”€â”€ test_integration/         # E2E tests (require API keys)
â”œâ”€â”€ config/                       # Configuration
â”‚   â”œâ”€â”€ markets/                  # Market configs
â”‚   â”‚   â”œâ”€â”€ proptech_de.yaml
â”‚   â”‚   â””â”€â”€ fashion_fr.yaml
â”‚   â”œâ”€â”€ models.yaml               # LLM model configs
â”‚   â””â”€â”€ notion_schemas.py         # Database schemas
â”œâ”€â”€ cache/                        # Disk cache (gitignored)
â”‚   â”œâ”€â”€ documents/                # Collected documents
â”‚   â”œâ”€â”€ topics/                   # Discovered topics
â”‚   â””â”€â”€ research/                 # Research reports
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ sessions/                 # Session narratives
â”‚   â”œâ”€â”€ decisions/                # ADRs
â”‚   â””â”€â”€ IMPLEMENTATION_PLAN.md    # 1,400+ line plan
â”œâ”€â”€ streamlit_app.py              # Main UI entry point
â”œâ”€â”€ requirements.txt              # Core dependencies
â”œâ”€â”€ requirements-topic-research.txt  # Topic research deps (langchain <1.0)
â””â”€â”€ PLAN.md                       # Original content creator plan
```

---

## Documentation

- **[ARCHITECTURE.md](ARCHITECTURE.md)** - System design, patterns, integration architecture
- **[CHANGELOG.md](CHANGELOG.md)** - Recent sessions (last 3-5 sessions)
- **[TASKS.md](TASKS.md)** - Current priorities, backlog, known issues
- **[docs/IMPLEMENTATION_PLAN.md](docs/IMPLEMENTATION_PLAN.md)** - Comprehensive 1,400+ line plan
- **[docs/sessions/](docs/sessions/)** - Session narratives (one per session)
- **[docs/decisions/](docs/decisions/)** - Architecture Decision Records (ADRs)

---

## License

Private project - All rights reserved

---

## Support

For issues and questions:
- **Known Issues**: See [TASKS.md](TASKS.md) section "Known Issues"
- **Session History**: See [CHANGELOG.md](CHANGELOG.md) for recent changes
- **Architecture**: See [ARCHITECTURE.md](ARCHITECTURE.md) for technical details
- **Implementation Plan**: See [docs/IMPLEMENTATION_PLAN.md](docs/IMPLEMENTATION_PLAN.md) for complete roadmap
