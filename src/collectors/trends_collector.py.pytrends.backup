"""
Trends Collector - Google Trends Data Collection

Features:
- pytrends for Google Trends API (unofficial but free)
- Trending searches (daily/realtime by region)
- Related queries (top/rising for keywords)
- Interest over time (search volume trends)
- Intelligent caching (1h for trending, 24h for interest)
- Conservative rate limiting (1 req/2sec - Google is strict!)
- Query health tracking with adaptive retry
- Regional targeting (DE, US, FR, etc.)

Usage:
    from src.collectors.trends_collector import TrendsCollector
    from src.database.sqlite_manager import DatabaseManager
    from src.processors.deduplicator import Deduplicator

    collector = TrendsCollector(
        config=config,
        db_manager=db_manager,
        deduplicator=deduplicator,
        region='DE'  # Germany
    )

    # Collect trending searches
    docs = collector.collect_trending_searches(pn='germany')

    # Collect related queries
    docs = collector.collect_related_queries(keywords=['PropTech'], query_type='top')

    # Collect interest over time
    docs = collector.collect_interest_over_time(keywords=['PropTech'])
"""

import json
import hashlib
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Literal
from dataclasses import dataclass, asdict
from enum import Enum

from pytrends.request import TrendReq
import pandas as pd

from src.utils.logger import get_logger
from src.models.document import Document

logger = get_logger(__name__)


class TrendsCollectorError(Exception):
    """Trends Collector related errors"""
    pass


class TrendType(str, Enum):
    """Types of trend queries"""
    TRENDING_SEARCHES = "trending_searches"
    RELATED_QUERIES = "related_queries"
    INTEREST_OVER_TIME = "interest_over_time"


@dataclass
class QueryHealth:
    """Track query reliability and health metrics"""
    query_id: str
    success_count: int = 0
    failure_count: int = 0
    consecutive_failures: int = 0
    last_success: Optional[datetime] = None
    last_failure: Optional[datetime] = None

    def record_success(self):
        """Record successful query"""
        self.success_count += 1
        self.consecutive_failures = 0
        self.last_success = datetime.now()

    def record_failure(self):
        """Record failed query"""
        self.failure_count += 1
        self.consecutive_failures += 1
        self.last_failure = datetime.now()

    def is_healthy(self, max_consecutive_failures: int = 5) -> bool:
        """Check if query is healthy"""
        return self.consecutive_failures < max_consecutive_failures


class TrendsCollector:
    """
    Google Trends collector with intelligent caching and rate limiting

    Features:
    - Trending searches (daily/realtime by region)
    - Related queries (top/rising for keywords)
    - Interest over time (search volume trends)
    - Conservative rate limiting (1 req/2sec)
    - Smart caching (1h trending, 24h interest)
    - Query health tracking
    """

    def __init__(
        self,
        config,
        db_manager,
        deduplicator,
        cache_dir: str = "cache/trends",
        region: str = "US",
        rate_limit: float = 0.5,  # Requests per second (default: 1 req/2sec)
        request_timeout: int = 10,
        max_consecutive_failures: int = 5
    ):
        """
        Initialize Trends Collector

        Args:
            config: Market configuration
            db_manager: Database manager for persistence
            deduplicator: Deduplicator for duplicate detection
            cache_dir: Directory for cache storage
            region: Default region for trends (ISO code: US, DE, FR, etc.)
            rate_limit: Requests per second (default 0.5 = 1 req/2sec)
            request_timeout: Request timeout in seconds
            max_consecutive_failures: Max failures before marking query unhealthy
        """
        self.config = config
        self.db_manager = db_manager
        self.deduplicator = deduplicator
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.region = region
        self.rate_limit = rate_limit
        self.request_timeout = request_timeout
        self.max_consecutive_failures = max_consecutive_failures

        # Internal state
        self._cache: Dict = {}  # In-memory cache
        self.query_health: Dict[str, QueryHealth] = {}
        self.last_request_time: Optional[float] = None

        # Statistics
        self._stats = {
            'total_queries': 0,
            'total_documents': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'failed_queries': 0,
        }

        # Load cache from disk
        self.load_cache()

        logger.info(
            "TrendsCollector initialized",
            region=region,
            rate_limit=rate_limit,
            cache_dir=str(self.cache_dir)
        )

    def collect_trending_searches(
        self,
        pn: str = 'united_states'
    ) -> List[Document]:
        """
        Collect trending searches for a region

        Args:
            pn: Region name (e.g., 'germany', 'united_states', 'france')

        Returns:
            List of Document objects for trending searches

        Raises:
            TrendsCollectorError: If collection fails
        """
        query_id = f"trending_searches_{pn}"

        # Check if query is healthy
        if self._should_skip_query(query_id):
            logger.warning("Skipping unhealthy query", query_id=query_id)
            return []

        # Check cache (1 hour TTL for trending searches)
        cache_key = f"trending_searches_{pn}"
        cached = self._get_from_cache(cache_key, ttl_hours=1)
        if cached:
            logger.info("Trending searches retrieved from cache", pn=pn)
            self._stats['cache_hits'] += 1
            return self._create_documents_from_cache(cached, f"trends_trending_searches_{pn}")

        self._stats['cache_misses'] += 1

        # Enforce rate limiting
        self._enforce_rate_limit()

        try:
            # Initialize pytrends
            # Note: Avoid retries/backoff_factor due to urllib3 compatibility issues
            pytrends = TrendReq(
                hl='en-US',
                tz=360,
                timeout=self.request_timeout
            )

            # Fetch trending searches
            logger.info("Fetching trending searches", pn=pn)
            df = pytrends.trending_searches(pn=pn)

            if df.empty:
                logger.warning("No trending searches found", pn=pn)
                self._record_query_success(query_id)
                return []

            # Cache results
            trends_data = []
            for idx, row in df.iterrows():
                # Use iloc for position-based indexing (avoid deprecation warning)
                title = row.iloc[0] if isinstance(row.iloc[0], str) else str(row.iloc[0])
                traffic = row.iloc[1] if len(row) > 1 else "N/A"
                trends_data.append({
                    'title': title,
                    'traffic': traffic
                })

            self._save_to_cache(cache_key, trends_data)

            # Create documents
            documents = []
            for trend in trends_data:
                doc = self._create_document(
                    source=f"trends_trending_searches_{pn}",
                    title=trend['title'],
                    content=f"Trending search: {trend['title']}\nTraffic: {trend['traffic']}",
                    metadata={'traffic': trend['traffic'], 'region': pn}
                )

                # Check for duplicates
                if self.deduplicator.is_duplicate(doc.content):
                    logger.debug("Skipping duplicate trend", title=doc.title)
                    continue

                documents.append(doc)

            self._stats['total_queries'] += 1
            self._stats['total_documents'] += len(documents)
            self._record_query_success(query_id)

            logger.info(
                "Trending searches collected",
                pn=pn,
                count=len(documents)
            )

            return documents

        except Exception as e:
            self._stats['failed_queries'] += 1
            self._record_query_failure(query_id)
            logger.error(
                "Failed to collect trending searches",
                pn=pn,
                error=str(e)
            )
            raise TrendsCollectorError(f"Failed to collect trending searches: {e}")

    def collect_related_queries(
        self,
        keywords: List[str],
        query_type: Literal['top', 'rising'] = 'top',
        timeframe: str = 'today 3-m'
    ) -> List[Document]:
        """
        Collect related queries for keywords

        Args:
            keywords: List of keywords to analyze
            query_type: 'top' or 'rising' queries
            timeframe: Time range (e.g., 'today 3-m', 'today 12-m')

        Returns:
            List of Document objects for related queries

        Raises:
            TrendsCollectorError: If collection fails
        """
        query_id = f"related_queries_{'_'.join(keywords)}"

        # Check cache (24 hour TTL)
        cache_key = f"related_queries_{query_type}_{'_'.join(keywords)}_{timeframe}"
        cached = self._get_from_cache(cache_key, ttl_hours=24)
        if cached:
            logger.info("Related queries retrieved from cache", keywords=keywords)
            self._stats['cache_hits'] += 1
            # Add query_type to cached data for proper reconstruction
            for item in cached:
                item['query_type'] = query_type
            return self._create_documents_from_cache(cached, "trends_related_queries")

        self._stats['cache_misses'] += 1

        # Enforce rate limiting
        self._enforce_rate_limit()

        try:
            # Initialize pytrends
            pytrends = TrendReq(
                hl='en-US',
                tz=360,
                timeout=self.request_timeout
            )

            # Build payload
            pytrends.build_payload(
                kw_list=keywords,
                cat=0,
                timeframe=timeframe,
                geo=self.region
            )

            # Fetch related queries
            logger.info(
                "Fetching related queries",
                keywords=keywords,
                query_type=query_type
            )
            related_dict = pytrends.related_queries()

            # Parse results
            queries_data = []
            for keyword, data in related_dict.items():
                if data[query_type] is not None and not data[query_type].empty:
                    df = data[query_type]
                    for idx, row in df.iterrows():
                        queries_data.append({
                            'keyword': keyword,
                            'query': row['query'],
                            'value': int(row['value']) if 'value' in row else 0
                        })

            # Cache results
            self._save_to_cache(cache_key, queries_data)

            # Create documents
            documents = []
            for query_data in queries_data:
                title_prefix = "Related query" if query_type == 'top' else "Rising query"
                doc = self._create_document(
                    source="trends_related_queries",
                    title=f"{title_prefix}: {query_data['query']}",
                    content=f"Related to: {query_data['keyword']}\nQuery: {query_data['query']}\nValue: {query_data['value']}",
                    metadata={
                        'parent_keyword': query_data['keyword'],
                        'query_type': query_type,
                        'value': query_data['value']
                    }
                )

                # Check for duplicates
                if self.deduplicator.is_duplicate(doc.content):
                    logger.debug("Skipping duplicate query", query=doc.title)
                    continue

                documents.append(doc)

            self._stats['total_queries'] += 1
            self._stats['total_documents'] += len(documents)

            logger.info(
                "Related queries collected",
                keywords=keywords,
                query_type=query_type,
                count=len(documents)
            )

            return documents

        except Exception as e:
            self._stats['failed_queries'] += 1
            logger.error(
                "Failed to collect related queries",
                keywords=keywords,
                error=str(e)
            )
            raise TrendsCollectorError(f"Failed to collect related queries: {e}")

    def collect_interest_over_time(
        self,
        keywords: List[str],
        timeframe: str = 'today 3-m'
    ) -> List[Document]:
        """
        Collect interest over time for keywords

        Args:
            keywords: List of keywords to analyze
            timeframe: Time range (e.g., 'today 3-m', '2025-01-01 2025-11-04')

        Returns:
            List of Document objects with interest trends

        Raises:
            TrendsCollectorError: If collection fails
        """
        query_id = f"interest_over_time_{'_'.join(keywords)}"

        # Check cache (24 hour TTL)
        cache_key = f"interest_over_time_{'_'.join(keywords)}_{timeframe}"
        cached = self._get_from_cache(cache_key, ttl_hours=24)
        if cached:
            logger.info("Interest over time retrieved from cache", keywords=keywords)
            self._stats['cache_hits'] += 1
            return self._create_documents_from_cache(cached, "trends_interest_over_time")

        self._stats['cache_misses'] += 1

        # Enforce rate limiting
        self._enforce_rate_limit()

        try:
            # Initialize pytrends
            pytrends = TrendReq(
                hl='en-US',
                tz=360,
                timeout=self.request_timeout
            )

            # Build payload
            pytrends.build_payload(
                kw_list=keywords,
                cat=0,
                timeframe=timeframe,
                geo=self.region
            )

            # Fetch interest over time
            logger.info(
                "Fetching interest over time",
                keywords=keywords,
                timeframe=timeframe
            )
            df = pytrends.interest_over_time()

            if df.empty:
                logger.warning("No interest data found", keywords=keywords)
                return []

            # Parse results
            interest_data = []
            for keyword in keywords:
                if keyword in df.columns:
                    values = df[keyword].dropna().tolist()
                    avg_interest = sum(values) / len(values) if values else 0

                    interest_data.append({
                        'keyword': keyword,
                        'average_interest': round(avg_interest, 2),
                        'max_interest': max(values) if values else 0,
                        'min_interest': min(values) if values else 0,
                        'data_points': len(values)
                    })

            # Cache results
            self._save_to_cache(cache_key, interest_data)

            # Create documents
            documents = []
            for data in interest_data:
                doc = self._create_document(
                    source="trends_interest_over_time",
                    title=f"Interest over time: {data['keyword']}",
                    content=f"Keyword: {data['keyword']}\nAverage interest: {data['average_interest']}\nMax: {data['max_interest']}\nMin: {data['min_interest']}\nData points: {data['data_points']}",
                    metadata=data
                )

                documents.append(doc)

            self._stats['total_queries'] += 1
            self._stats['total_documents'] += len(documents)

            logger.info(
                "Interest over time collected",
                keywords=keywords,
                count=len(documents)
            )

            return documents

        except Exception as e:
            self._stats['failed_queries'] += 1
            logger.error(
                "Failed to collect interest over time",
                keywords=keywords,
                error=str(e)
            )
            raise TrendsCollectorError(f"Failed to collect interest over time: {e}")

    def _create_document(
        self,
        source: str,
        title: str,
        content: str,
        metadata: Optional[Dict] = None,
        published_at: Optional[datetime] = None
    ) -> Document:
        """Create Document from trend data"""
        doc_id = self._generate_document_id(source, title)

        # Generate source URL for Google Trends
        # For trends, we create a synthetic URL since trends don't have traditional URLs
        keyword_slug = title.lower().replace(' ', '-')[:50]
        source_url = f"https://trends.google.com/trends/explore?q={keyword_slug}"
        canonical_url = self.deduplicator.get_canonical_url(source_url)

        # Use current time if no published_at provided
        if published_at is None:
            published_at = datetime.now()

        return Document(
            id=doc_id,
            source=source,
            source_url=source_url,
            canonical_url=canonical_url,
            title=title,
            content=content,
            language=self.config.market.language,
            domain=self.config.market.domain,
            market=self.config.market.market,
            vertical=self.config.market.vertical,
            published_at=published_at,
            fetched_at=datetime.now(),
            content_hash=self.deduplicator.compute_content_hash(content)
        )

    def _generate_document_id(self, source: str, title: str) -> str:
        """Generate unique document ID"""
        unique_string = f"{source}:{title}"
        return hashlib.md5(unique_string.encode()).hexdigest()

    def _get_from_cache(
        self,
        cache_key: str,
        ttl_hours: int
    ) -> Optional[List[Dict]]:
        """Get data from cache if not expired"""
        if cache_key not in self._cache:
            return None

        cached = self._cache[cache_key]
        cache_time = cached.get('timestamp')

        if not cache_time:
            return None

        # Check if expired
        if datetime.now() - cache_time > timedelta(hours=ttl_hours):
            del self._cache[cache_key]
            return None

        return cached.get('data')

    def _save_to_cache(self, cache_key: str, data: List[Dict]):
        """Save data to cache"""
        self._cache[cache_key] = {
            'timestamp': datetime.now(),
            'data': data
        }

    def _create_documents_from_cache(
        self,
        cached_data: List[Dict],
        source: str
    ) -> List[Document]:
        """Create documents from cached data"""
        documents = []

        for data in cached_data:
            # Reconstruct document from cached data
            if 'title' in data:
                # Trending searches format
                doc = self._create_document(
                    source=source,
                    title=data['title'],
                    content=f"Trending search: {data['title']}\nTraffic: {data.get('traffic', 'N/A')}",
                    metadata=data
                )
            elif 'query' in data:
                # Related queries format
                query_type = data.get('query_type', 'top')
                title_prefix = "Related query" if query_type == 'top' else "Rising query"
                doc = self._create_document(
                    source=source,
                    title=f"{title_prefix}: {data['query']}",
                    content=f"Related to: {data.get('keyword', 'N/A')}\nQuery: {data['query']}\nValue: {data.get('value', 0)}",
                    metadata=data
                )
            elif 'keyword' in data and 'average_interest' in data:
                # Interest over time format
                doc = self._create_document(
                    source=source,
                    title=f"Interest over time: {data['keyword']}",
                    content=f"Keyword: {data['keyword']}\nAverage interest: {data['average_interest']}\nMax: {data.get('max_interest', 0)}\nMin: {data.get('min_interest', 0)}\nData points: {data.get('data_points', 0)}",
                    metadata=data
                )
            else:
                continue

            # Check for duplicates
            if self.deduplicator.is_duplicate(doc.content):
                logger.debug("Skipping duplicate (from cache)", title=doc.title)
                continue

            documents.append(doc)

        return documents

    def _enforce_rate_limit(self):
        """Enforce rate limiting between requests"""
        if self.last_request_time is None:
            self.last_request_time = time.time()
            return

        elapsed = time.time() - self.last_request_time
        required_delay = 1.0 / self.rate_limit

        if elapsed < required_delay:
            sleep_time = required_delay - elapsed
            logger.debug("Rate limiting", sleep_time=sleep_time)
            time.sleep(sleep_time)

        self.last_request_time = time.time()

    def _should_skip_query(self, query_id: str) -> bool:
        """Check if query should be skipped due to poor health"""
        if query_id not in self.query_health:
            return False

        health = self.query_health[query_id]
        return not health.is_healthy(self.max_consecutive_failures)

    def _record_query_success(self, query_id: str):
        """Record successful query"""
        if query_id not in self.query_health:
            self.query_health[query_id] = QueryHealth(query_id=query_id)

        self.query_health[query_id].record_success()

    def _record_query_failure(self, query_id: str):
        """Record failed query"""
        if query_id not in self.query_health:
            self.query_health[query_id] = QueryHealth(query_id=query_id)

        self.query_health[query_id].record_failure()

    def get_statistics(self) -> Dict:
        """Get collection statistics"""
        return self._stats.copy()

    def save_cache(self):
        """Save cache to disk"""
        cache_file = self.cache_dir / "trends_cache.json"

        # Prepare cache for serialization
        serializable_cache = {}
        for key, value in self._cache.items():
            serializable_cache[key] = {
                'timestamp': value['timestamp'].isoformat(),
                'data': value['data']
            }

        try:
            with open(cache_file, 'w') as f:
                json.dump(serializable_cache, f, indent=2)
            logger.info("Cache saved", file=str(cache_file))
        except Exception as e:
            logger.error("Failed to save cache", error=str(e))

    def load_cache(self):
        """Load cache from disk"""
        cache_file = self.cache_dir / "trends_cache.json"

        if not cache_file.exists():
            return

        try:
            with open(cache_file, 'r') as f:
                serialized_cache = json.load(f)

            # Deserialize cache
            for key, value in serialized_cache.items():
                self._cache[key] = {
                    'timestamp': datetime.fromisoformat(value['timestamp']),
                    'data': value['data']
                }

            logger.info("Cache loaded", entries=len(self._cache))
        except Exception as e:
            logger.error("Failed to load cache", error=str(e))
