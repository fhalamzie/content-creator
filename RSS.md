# RSS Feed Discovery System - Implementation Plan

## üìã Overview

**Problem:** RSS collector is currently disabled because it needs actual RSS feed URLs (e.g., `https://techcrunch.com/feed/`), but LLM only suggests feed names (e.g., "TechCrunch").

**Solution:** Build an automated RSS feed discovery and curation system with a database of 10K-50K RSS feeds organized by domain and vertical.

**Goal:** Enable dynamic RSS-based topic discovery across any industry/niche without manual feed configuration.

---

## ‚úÖ Current Status (Phase 1 Complete!)

**What We Have:**
- ‚úÖ **1,037 RSS feeds** across **22 domains** and **85 verticals**
- ‚úÖ **Dynamic feed generation** for unlimited topics (Bing News, Google News, Reddit)
- ‚úÖ **OPML parser** for importing feed collections
- ‚úÖ **JSON database** with CRUD operations and quality scoring
- ‚úÖ **Feed discovery tools** (pattern-based + HTML autodiscovery)

**Feed Sources:**
1. awesome-rss-feeds (GitHub): **518 feeds**
2. RSS-Link-Database-2024 (GitHub): **519 feeds**
3. Dynamic generation: **Unlimited feeds** for any keyword

**Top Domains:**
- Technology: 336 feeds
- Lifestyle: 160 feeds
- News: 120 feeds
- Entertainment: 98 feeds
- Sports: 59 feeds

**Timeline:** Phase 1 completed in ~4 hours
**Cost:** $0 (100% FREE)

---

## üéØ Success Metrics

- **Coverage:** ‚úÖ 1,037 validated RSS feeds (target: 1,000+)
- **Verticals:** ‚úÖ 85 verticals (target: 100+)
- **Quality:** üîÑ Average feed quality score 0.5 (target: >0.6 after validation)
- **Freshness:** üîÑ To be validated in Phase 3
- **Integration:** ‚è≥ Next step: Integrate with Hybrid Research Orchestrator
- **Cost:** ‚úÖ $0 (target: <$10/month)
- **Time:** ‚úÖ Phase 1 complete in 4 hours (target: 1-2 days)

---

## üîç Discovery Methods Research

### Method 1: AllTop OPML Exports ‚≠ê‚≠ê RECOMMENDED (Phase 2)

**How it works:** AllTop provides free OPML exports for every category by appending `/opml` to category URLs.

**Example:**
- Category: `https://alltop.com/tech`
- OPML: `https://alltop.com/tech/opml`

**Estimated Coverage:** 10,000-50,000 feeds across hundreds of categories

**Pros:**
- ‚úÖ 100% FREE
- ‚úÖ Legal (public OPML exports)
- ‚úÖ No scraping needed
- ‚úÖ Standard OPML format
- ‚úÖ Hundreds of categories (tech, business, health, etc.)

**Cons:**
- ‚ùå Need to discover all category URLs
- ‚ùå May have some stale feeds (requires validation)

**Implementation:**
1. Scrape AllTop homepage to find all category links
2. For each category, download `/opml` endpoint
3. Parse OPML XML to extract feed URLs
4. Validate each feed (check parseable, freshness, quality)
5. Import to database with categorization

**Cost:** FREE
**Time:** 3-5 days (includes validation)

---

### Method 2: GitHub awesome-rss-feeds ‚≠ê‚≠ê‚≠ê RECOMMENDED (Phase 1)

**Source:** https://github.com/plenaryapp/awesome-rss-feeds

**What you get:**
- ~500 curated high-quality feeds
- OPML files included in repo
- Organized by topics (news, tech, business, science, etc.)
- Community-maintained (actively updated)

**Pros:**
- ‚úÖ 100% FREE and open source
- ‚úÖ High-quality curated feeds
- ‚úÖ Already validated by community
- ‚úÖ Ready to import (OPML format)
- ‚úÖ Quick bootstrap (can implement in 1-2 hours)

**Cons:**
- ‚ùå Only ~500 feeds (need more for comprehensive coverage)

**Implementation:**
1. Clone GitHub repo
2. Parse OPML files from repo
3. Import feeds to database
4. Test integration with Hybrid Research Orchestrator

**Cost:** FREE
**Time:** 1-2 hours

---

### Method 3: Internet Archive OPML Collections

**Source:** https://archive.org/details/OPMLdirectoryofRSSfeeds

**What you get:**
- Historical OPML collections
- Various topic categories
- Free downloads

**Pros:**
- ‚úÖ FREE
- ‚úÖ No restrictions

**Cons:**
- ‚ùå Likely outdated (may have many dead feeds)
- ‚ùå Requires extensive validation

**Recommendation:** Use as fallback if AllTop doesn't provide enough coverage.

---

### Method 4: Pattern-Based Discovery (Fill Gaps)

**How it works:** Try common RSS URL patterns on known domains.

**Patterns to try:**
```
/feed/, /rss/, /rss.xml, /feed.xml, /atom.xml,
/feeds/posts/default, /blog/feed/, /news/rss/
```

**Pros:**
- ‚úÖ FREE
- ‚úÖ Finds feeds not in directories
- ‚úÖ Works on 60-70% of websites

**Cons:**
- ‚ùå Need to know which domains to check
- ‚ùå Slower than directory imports

**Use case:** Fill gaps in verticals with <10 feeds after AllTop import.

**Status:** Already implemented in `src/collectors/rss_feed_discoverer.py`

---

### Method 5: HTML Autodiscovery (Fill Gaps)

**How it works:** Parse HTML for `<link rel="alternate" type="application/rss+xml">` tags.

**Pros:**
- ‚úÖ FREE
- ‚úÖ Most reliable (90%+ success if tag exists)
- ‚úÖ Gets official feed URL

**Cons:**
- ‚ùå Slower (requires full HTML fetch)

**Use case:** Validate pattern-based discoveries or find feeds on known domains.

**Status:** Already implemented in `src/collectors/rss_feed_discoverer.py`

---

### Method 6: LLM + Google Search (Future Enhancement)

**How it works:**
1. LLM generates search queries for domain/vertical
2. Gemini grounding finds relevant websites
3. Apply pattern + HTML discovery to found sites

**Pros:**
- ‚úÖ Discovers niche/emerging sources
- ‚úÖ Customizable to any domain/vertical

**Cons:**
- ‚ùå Costs API calls (~$0.01-0.02 per vertical)
- ‚ùå Slower

**Use case:** Discover feeds for very niche verticals not covered by directories.

**Status:** Already implemented in `scripts/build_rss_database.py`

---

### Method 7: Feedspot Database (Paid Option)

**What you get:**
- 500K RSS feeds
- 1500 niche categories
- CSV/API export
- Official API access

**Cost:** Unknown (estimated $100-500 one-time or $50-100/month)

**Recommendation:** Only if free methods don't provide sufficient coverage.

---

## üöÄ Recommended 3-Phase Strategy

### **Phase 1: Quick Bootstrap (1-2 days)** ‚úÖ START HERE

**Objective:** Get 500 quality feeds working immediately.

**Steps:**
1. Download awesome-rss-feeds from GitHub
2. Build OPML parser
3. Import feeds to database (`src/config/rss_feeds_database.json`)
4. Integrate with RSS collector in Hybrid Research Orchestrator
5. Test with pipeline automation

**Deliverables:**
- ‚úÖ OPML parser (`src/collectors/opml_parser.py`)
- ‚úÖ 500 validated feeds in database
- ‚úÖ RSS collector using database feeds
- ‚úÖ End-to-end test with German topic discovery

**Success Criteria:**
- RSS collector returns >0 topics
- Topics are translated to German
- Topics pass quality threshold (>0.6 score)

**Timeline:** 1-2 days
**Cost:** FREE

---

### **Phase 2A: Dynamic Feed Generation (DONE ‚úÖ)**

**Objective:** Enable infinite on-demand RSS feeds for any topic.

**Implemented:**
- ‚úÖ Bing News RSS generator (`DynamicFeedGenerator`)
- ‚úÖ Google News RSS generator
- ‚úÖ Reddit RSS generator
- ‚úÖ Keyword-based feed generation
- ‚úÖ Combined query feeds (OR/AND operators)

**Usage:**
```python
from src.collectors.dynamic_feed_generator import DynamicFeedGenerator

gen = DynamicFeedGenerator()

# Generate Bing News feed for PropTech in German
feed = gen.generate_bing_news_feed("PropTech", language="de", region="DE")

# Generate feeds for multiple keywords
feeds = gen.generate_feeds_for_keywords(
    keywords=["PropTech", "Smart Buildings", "IoT"],
    sources=["bing", "google"],
    language="de",
    region="DE"
)
# Returns 6 feeds (3 keywords √ó 2 sources)
```

**Outcome:** Unlimited RSS feeds for any search query!

**Timeline:** COMPLETE (2 hours)
**Cost:** FREE

---

### **Phase 2B: AllTop OPML Import (FUTURE)**

**Objective:** Scale to 10,000+ feeds across 100+ verticals.

**Steps:**
1. Discover all AllTop category URLs (scrape https://alltop.com)
2. Download OPML for each category (append `/opml` to category URL)
   - Example: `https://alltop.com/tech/opml`
3. Parse and extract feed URLs
4. Validate all feeds:
   - Check if parseable
   - Check freshness (last updated)
   - Calculate quality score
   - Remove feeds with score <0.5
5. Import to database with proper categorization
6. Add feed selection UI (browse by domain/vertical)

**Deliverables:**
- [ ] AllTop category scraper
- [ ] 10,000+ validated feeds
- [ ] UI to browse feeds by category
- [ ] Automatic feed selection based on keywords

**Success Criteria:**
- 100+ verticals with 20-50 feeds each
- Average quality score >0.6
- 90% of feeds updated within 30 days

**Timeline:** 3-5 days
**Cost:** FREE

**Status:** Not started (optional enhancement)

---

### **Phase 2C: RSSHub Integration (FUTURE)**

**Objective:** Access 300+ platforms without native RSS feeds.

**What is RSSHub:**
RSSHub is an open-source RSS feed generator that creates feeds for 300+ platforms:
- Social media (Twitter, Instagram, Reddit, YouTube)
- News sites (NY Times, BBC, Guardian, etc.)
- E-commerce (ProductHunt, HackerNews)
- Podcasts, GitHub, Medium, Substack
- **Everything without native RSS!**

**RSSHub Routes Examples:**
```
Reddit:        https://rsshub.app/reddit/r/saas
Twitter:       https://rsshub.app/twitter/user/elonmusk
YouTube:       https://rsshub.app/youtube/user/@mkbhd
ProductHunt:   https://rsshub.app/producthunt/today
HackerNews:    https://rsshub.app/hackernews/best
GitHub:        https://rsshub.app/github/trending/python
Medium:        https://rsshub.app/medium/@username
Instagram:     https://rsshub.app/instagram/user/username
```

**Implementation Options:**

**Option A: Use Public Instance**
- URL: `https://rsshub.app`
- Pros: FREE, no setup
- Cons: Rate limits, reliability concerns

**Option B: Self-Host (Recommended for Production)**
```bash
# Docker deployment
docker run -d --name rsshub -p 1200:1200 diygod/rsshub

# Then access at http://localhost:1200
```

**Integration:**
```python
from src.collectors.rss_feed_generator import RSSHubGenerator

generator = RSSHubGenerator(base_url="https://rsshub.app")

# Generate Instagram feed
feed = generator.generate_feed("instagram", "user", "username")

# Generate Twitter feed
feed = generator.generate_feed("twitter", "user", "elonmusk")

# Generate ProductHunt feed
feed = generator.generate_feed("producthunt", "today")
```

**Deliverables:**
- [ ] RSSHubGenerator class
- [ ] Support for 50+ most useful routes
- [ ] Self-hosting deployment guide
- [ ] Integration with Hybrid Research Orchestrator

**Success Criteria:**
- Access to 300+ platforms
- Self-hosted instance running
- <1s feed generation time

**Timeline:** 2-3 days
**Cost:** FREE (self-hosting) or ~$5-10/month (managed hosting)

**Status:** Not started (optional enhancement)

**Documentation:** https://docs.rsshub.app/

---

### **Phase 3: Enhancement & Maintenance (Ongoing)**

**Objective:** Maintain database quality and fill gaps.

**Steps:**
1. **Weekly Validation:**
   - Check feed freshness
   - Remove stale feeds (not updated in 90 days)
   - Update quality scores

2. **Monthly Discovery:**
   - Identify verticals with <10 feeds
   - Use web search + discovery to find more sources
   - Add new feeds to database

3. **Analytics:**
   - Track which feeds produce best topics
   - Prioritize high-performing feeds
   - Remove low-performing feeds

4. **User Feedback:**
   - Allow users to suggest feeds
   - Add manual feed addition UI
   - User voting on feed quality

**Deliverables:**
- ‚úÖ Automated validation script
- ‚úÖ Feed analytics dashboard
- ‚úÖ User feed suggestion form

**Success Criteria:**
- 95% of feeds updated within 30 days
- Average quality score >0.65
- User-suggested feeds added monthly

**Timeline:** 1-2 hours/week
**Cost:** <$5/month (for optional web discovery)

---

## üìä Database Architecture

### **Schema Design**

**File:** `src/config/rss_feeds_database.json`

**Structure:**
```json
{
  "domains": {
    "technology": {
      "saas": [
        {
          "url": "https://techcrunch.com/feed",
          "source_url": "https://techcrunch.com",
          "title": "TechCrunch",
          "description": "The latest technology news",
          "discovery_method": "opml",
          "quality_score": 0.92,
          "last_updated": "2025-11-15T10:00:00",
          "article_count": 250,
          "is_valid": true,
          "discovered_at": "2025-11-16T08:00:00",
          "last_validated": "2025-11-16T08:30:00"
        }
      ],
      "ai": [...],
      "cybersecurity": [...]
    },
    "business": {
      "entrepreneurship": [...],
      "marketing": [...],
      "finance": [...]
    },
    "health": {
      "medicine": [...],
      "fitness": [...],
      "nutrition": [...]
    }
  },
  "metadata": {
    "version": "1.0",
    "created_at": "2025-11-16T08:00:00",
    "last_updated": "2025-11-16T10:00:00",
    "total_feeds": 12450,
    "total_domains": 15,
    "total_verticals": 125
  }
}
```

### **Quality Scoring Formula**

**Score Range:** 0.0 - 1.0

**Components:**
- **Parseable (40%):** Feed is valid RSS/Atom and can be parsed
- **Article Count (30%):** Number of articles (max score at 50+ articles)
- **Freshness (20%):** Recency of last update (max score if updated in last 7 days)
- **Metadata (10%):** Has title and description

**Minimum Threshold:** 0.5 (feeds below this are discarded)

**Example Calculation:**
```
Feed: TechCrunch
- Parseable: 1.0 (yes) ‚Üí 0.40
- Article count: 250 ‚Üí 1.0 ‚Üí 0.30
- Last updated: 1 day ago ‚Üí 1.0 ‚Üí 0.20
- Has title+desc: yes ‚Üí 0.10
Total: 1.0 (perfect score)
```

### **Storage Options**

**Phase 1-2: JSON File**
- Simple, no setup
- Version control friendly
- Fast for <10K feeds
- No dependencies

**Phase 3: Migrate to SQLite (if >10K feeds)**
- Fast queries with indexes
- Complex filtering
- Scales to 100K+ feeds
- Requires schema migrations

**Recommendation:** Start with JSON, migrate to SQLite only if performance degrades.

---

## üîó Integration with Hybrid Research Orchestrator

### **Current RSS Collector Flow**

**File:** `src/orchestrator/hybrid_research_orchestrator.py`

**Current limitation:**
```python
# RSS disabled - needs feed URLs, not names
rss_feed_urls = config.get("rss_feed_urls", [])
if not rss_feed_urls:
    logger.info("rss_feeds_disabled", reason="no_feed_urls")
    return []
```

### **Updated Flow (After Phase 1)**

**Option A: Automatic Feed Selection** ‚≠ê RECOMMENDED

```python
# NEW: Auto-select feeds based on keywords
from src.collectors.rss_feed_database import RSSFeedDatabase

db = RSSFeedDatabase()

# Extract keywords from customer site or config
keywords = config.get("keywords", [])  # ["PropTech", "Smart Buildings", "IoT"]

# Find relevant feeds
feeds = db.find_feeds_by_keywords(
    keywords=keywords,
    min_quality_score=0.6,
    limit=15  # Top 15 feeds
)

# Collect topics from feeds
topics = await rss_collector.collect(feed_urls=feeds)
```

**Option B: Manual Feed Selection (UI)**

Add UI in Pipeline Automation page:
```python
# Browse feeds by domain/vertical
domain = st.selectbox("Domain", db.get_domains())
vertical = st.selectbox("Vertical", db.get_verticals(domain))
feeds = db.get_feeds(domain=domain, vertical=vertical, min_quality_score=0.6)

# Show feeds with quality scores
selected_feeds = st.multiselect(
    "Select RSS Feeds",
    options=[(f["url"], f"{f['title']} ({f['quality_score']:.2f})") for f in feeds]
)

# Use selected feeds
topics = await rss_collector.collect(feed_urls=selected_feeds)
```

**Option C: Hybrid (Smart Defaults + User Override)**

```python
# Auto-select based on keywords
auto_feeds = db.find_feeds_by_keywords(keywords, limit=15)

# Allow user to add/remove feeds
with st.expander("RSS Feed Selection (Auto-selected based on keywords)"):
    selected_feeds = st.multiselect(
        "Feeds",
        options=all_feeds,
        default=auto_feeds
    )

topics = await rss_collector.collect(feed_urls=selected_feeds)
```

**Recommendation:** Option A for automation, add Option C later for power users.

---

## üõ†Ô∏è Implementation Files

### **New Files to Create**

1. **`src/collectors/opml_parser.py`**
   - Parse OPML XML files
   - Extract feed URLs, titles, descriptions
   - Convert to RSSFeed objects

2. **`src/collectors/rss_feed_database.py`** ‚úÖ ALREADY CREATED
   - Database manager with CRUD operations
   - Query feeds by domain/vertical/keywords
   - Export feeds list

3. **`src/collectors/rss_feed_discoverer.py`** ‚úÖ ALREADY CREATED
   - Pattern-based discovery
   - HTML autodiscovery
   - Feed validation and quality scoring

4. **`scripts/import_awesome_feeds.py`** (Phase 1)
   - Clone awesome-rss-feeds repo
   - Parse OPML files
   - Import to database

5. **`scripts/import_alltop_feeds.py`** (Phase 2)
   - Scrape AllTop categories
   - Download OPML for each category
   - Validate and import feeds

6. **`scripts/validate_feeds.py`** (Phase 3)
   - Check feed freshness
   - Update quality scores
   - Remove stale feeds

### **Files to Modify**

1. **`src/orchestrator/hybrid_research_orchestrator.py`**
   - Add database-based feed selection
   - Replace hardcoded feed URLs with dynamic queries
   - Add keyword-based feed matching

2. **`src/ui/pages/pipeline_automation.py`**
   - Add feed selection UI (Option C)
   - Show feed quality scores
   - Allow manual feed addition

---

## üìÖ Detailed Timeline

### **Phase 1: Quick Bootstrap (1-2 days)**

**Day 1 (4 hours):**
- [ ] Create `src/collectors/opml_parser.py`
- [ ] Create `scripts/import_awesome_feeds.py`
- [ ] Clone awesome-rss-feeds repo
- [ ] Parse and import ~500 feeds to database
- [ ] Test database CRUD operations

**Day 2 (4 hours):**
- [ ] Modify RSS collector to use database
- [ ] Add automatic feed selection based on keywords
- [ ] Test with Pipeline Automation
- [ ] Validate German translation works with RSS topics
- [ ] Document results

**Deliverable:** 500 working feeds, RSS collector enabled

---

### **Phase 2: Scale with AllTop (3-5 days)**

**Day 1 (6 hours):**
- [ ] Create `scripts/import_alltop_feeds.py`
- [ ] Scrape AllTop homepage for category links
- [ ] Test OPML download from 5 sample categories
- [ ] Validate OPML parsing

**Day 2-3 (12 hours):**
- [ ] Download OPML for all categories
- [ ] Parse and extract feed URLs
- [ ] Validate 10,000+ feeds (concurrent validation)
- [ ] Calculate quality scores

**Day 4 (6 hours):**
- [ ] Import feeds to database
- [ ] Remove duplicates and low-quality feeds
- [ ] Organize by domain/vertical

**Day 5 (6 hours):**
- [ ] Add feed browsing UI
- [ ] Test with multiple verticals
- [ ] Performance optimization
- [ ] Documentation

**Deliverable:** 10,000+ validated feeds across 100+ verticals

---

### **Phase 3: Maintenance (Ongoing)**

**Weekly (1 hour):**
- [ ] Run `scripts/validate_feeds.py`
- [ ] Check feed freshness
- [ ] Remove stale feeds (>90 days)
- [ ] Review analytics

**Monthly (1-2 hours):**
- [ ] Identify verticals with <10 feeds
- [ ] Discover new feeds for gaps
- [ ] Add user-suggested feeds
- [ ] Update documentation

---

## üí∞ Cost Analysis

### **Phase 1: Bootstrap**
- GitHub repo download: FREE
- OPML parsing: FREE
- Feed validation (500 feeds): FREE
- **Total: $0**

### **Phase 2: AllTop Import**
- AllTop scraping: FREE
- OPML downloads (100s of categories): FREE
- Feed validation (10K feeds): FREE
- **Total: $0**

### **Phase 3: Maintenance**
- Weekly validation: FREE
- Monthly discovery (optional web search): ~$1-2
- **Total: <$5/month**

### **Optional: Feedspot Paid Database**
- 500K feeds, 1500 categories
- Estimated cost: $100-500 one-time or $50-100/month
- **Recommendation:** Only if free methods insufficient

---

## üéØ Success Criteria

### **Phase 1 Complete:**
- ‚úÖ 500 feeds in database
- ‚úÖ RSS collector enabled and working
- ‚úÖ Topics discovered from RSS feeds
- ‚úÖ German translation working
- ‚úÖ Topics pass quality threshold (>0.6)

### **Phase 2 Complete:**
- ‚úÖ 10,000+ feeds in database
- ‚úÖ 100+ verticals covered
- ‚úÖ Average quality score >0.6
- ‚úÖ 90% of feeds updated within 30 days
- ‚úÖ Feed browsing UI working
- ‚úÖ Automatic feed selection working

### **Phase 3 Complete:**
- ‚úÖ Automated weekly validation
- ‚úÖ 95% of feeds fresh (<30 days)
- ‚úÖ Feed analytics tracking
- ‚úÖ User feedback mechanism
- ‚úÖ <5% stale feed rate

---

## üö® Risks & Mitigations

### **Risk 1: AllTop OPML endpoints removed or restricted**

**Mitigation:**
- Start with awesome-rss-feeds (proven to work)
- Have fallback to Internet Archive collections
- Keep web discovery method ready

### **Risk 2: Many feeds are stale/dead**

**Mitigation:**
- Implement robust validation (check parseable, freshness)
- Set minimum quality threshold (0.5)
- Regular cleanup (remove feeds >90 days old)

### **Risk 3: Database grows too large (>10K feeds)**

**Mitigation:**
- Start with JSON (fast for <10K feeds)
- Migrate to SQLite if performance degrades
- Add pagination in UI
- Implement feed ranking (show top feeds first)

### **Risk 4: RSS topics don't pass quality threshold**

**Mitigation:**
- Translate topics to target language
- Adjust quality scoring weights
- Combine with other collectors (LLM, Trends, Autocomplete)

---

## ‚úÖ Phase B Integration: Automated Competitor Feed Discovery (COMPLETE!)

### **Overview**

Phase B automatically discovers RSS feeds from competitor websites during the Hybrid Research Orchestrator's competitor analysis stage (Stage 2).

**Key Benefits:**
- üéØ **Niche-Specific Content:** Discovers industry blogs not in mainstream databases
- ü§ñ **Fully Automated:** Runs during existing competitor research workflow
- üí∞ **FREE:** Uses free Gemini API for feed categorization
- üìà **Continuous Growth:** Database grows automatically as you research new niches

### **Implementation**

**File:** `src/orchestrator/hybrid_research_orchestrator.py`

**New Method:** `discover_competitor_feeds()`

```python
async def discover_competitor_feeds(
    self,
    competitor_urls: List[str],
    hint_domain: Optional[str] = None,
    hint_vertical: Optional[str] = None
) -> Dict:
    """
    Phase B: Discover RSS feeds from competitor websites.

    Uses automated feed discovery to find and add competitor RSS feeds
    to the database for future topic collection.

    Returns:
        - feeds_discovered: Number of feeds found
        - feeds_added: Number of feeds added to database
        - feeds: List of discovered feed objects
        - cost: $0 (free Gemini for categorization)
    """
```

**Integration Point:**

Modified `research_competitors()` to include optional feed discovery:

```python
async def research_competitors(
    self,
    keywords: List[str],
    customer_info: Dict,
    max_competitors: int = 10,
    discover_feeds: bool = False  # NEW PARAMETER
) -> Dict:
    """
    Stage 2: Competitor/market research + optional RSS feed discovery.

    When discover_feeds=True:
    1. Identify competitors using Gemini + Google Search
    2. Extract competitor URLs
    3. Discover RSS feeds from competitor websites
    4. Auto-categorize and add to database
    """
```

### **Usage Example**

**Enable in Pipeline:**

```python
# Initialize orchestrator
orchestrator = HybridResearchOrchestrator(
    enable_tavily=True,
    enable_searxng=True,
    enable_gemini=True
)

# Run pipeline with competitor feed discovery
result = await orchestrator.run_pipeline(
    website_url="https://customer-website.com",
    customer_info={
        "market": "Germany",
        "vertical": "PropTech",
        "language": "de",
        "domain": "Real Estate"
    },
    max_topics_to_research=5,
    discover_competitor_feeds=True  # ENABLE PHASE B
)

# Check discovered feeds
if "rss_feeds" in result["competitor_data"]:
    feeds_added = result["competitor_data"]["rss_feeds"]["feeds_added"]
    print(f"‚úÖ Added {feeds_added} new RSS feeds to database!")
```

**Manual Discovery:**

```python
# Discover feeds from specific competitor URLs
competitor_urls = [
    "https://competitor1.com",
    "https://competitor2.com",
    "https://competitor3.com"
]

feed_result = await orchestrator.discover_competitor_feeds(
    competitor_urls=competitor_urls,
    hint_domain="technology",
    hint_vertical="proptech"
)

print(f"Discovered: {feed_result['feeds_discovered']} feeds")
print(f"Added to DB: {feed_result['feeds_added']} feeds")
```

### **How It Works**

**Step-by-Step Flow:**

1. **Competitor Research (Stage 2)**
   - Gemini API finds 10 competitors in the niche
   - Extracts competitor URLs

2. **Feed Discovery (Pattern + HTML)**
   - Tries common RSS patterns (`/feed/`, `/rss/`, `/atom.xml`)
   - Scans HTML for `<link rel="alternate">` tags
   - Validates each discovered feed

3. **Quality Scoring**
   - Parseable: 40%
   - Article count: 30%
   - Freshness: 20%
   - Metadata: 10%
   - Minimum threshold: 0.6

4. **Auto-Categorization**
   - Gemini API categorizes feed into domain/vertical
   - Uses hint_domain and hint_vertical for better accuracy

5. **Database Storage**
   - Adds feed to `src/config/rss_feeds_database.json`
   - Deduplicates by URL
   - Available for future topic discovery

### **Expected Growth Rate**

**Per Competitor Analysis Run:**
- Competitors analyzed: 10
- Feeds per competitor: 0-3 (avg ~1)
- **Expected new feeds: 5-15 per run**

**Monthly Growth (assuming 20 analyses/month):**
- **100-300 new feeds/month**
- **1,000+ feeds in 6-12 months**

### **Files Created/Modified**

**New Files:**
- ‚úÖ `src/collectors/automated_feed_discovery.py` - Core automation system
- ‚úÖ `scripts/grow_rss_database.py` - Manual/scheduled growth tool

**Modified Files:**
- ‚úÖ `src/orchestrator/hybrid_research_orchestrator.py` - Added Phase B integration

---

## üìù Next Steps

### **Completed:**
1. ‚úÖ Create `src/collectors/opml_parser.py`
2. ‚úÖ Create `scripts/import_awesome_feeds.py`
3. ‚úÖ Download and import awesome-rss-feeds (518 feeds)
4. ‚úÖ Import RSS-Link-Database-2024 (519 feeds)
5. ‚úÖ Create dynamic feed generator (Bing News, Google News, Reddit)
6. ‚úÖ Build automated feed discovery system (Phase B)
7. ‚úÖ Integrate with Hybrid Research Orchestrator

### **Current Focus:**
1. Test Phase B integration end-to-end
2. Monitor feed quality and database growth
3. Document Phase C (continuous automated growth)

### **Future Enhancements:**
- Phase C: Daily automated growth (100-200 feeds/day)
- Phase 2B: AllTop OPML import (10,000-50,000 feeds)
- Phase 2C: RSSHub integration (300+ platforms)

---

## üìö References

- **awesome-rss-feeds:** https://github.com/plenaryapp/awesome-rss-feeds
- **AllTop:** https://alltop.com
- **AllTop OPML:** Append `/opml` to any category URL
- **Internet Archive OPML:** https://archive.org/details/OPMLdirectoryofRSSfeeds
- **Feedspot:** https://www.feedspot.com (paid option)

---

## üìñ Related Documentation

- [ARCHITECTURE.md](ARCHITECTURE.md) - Current MVP architecture
- [TARGET_ARCHITECTURE.md](TARGET_ARCHITECTURE.md) - Production SaaS plan
- [TASKS.md](TASKS.md) - Current priorities

---

## üåç Multilingual Content Strategy

### **Adaptive Hybrid Approach: Configurable English/Local Ratio**

**Core Principle:** Research using both English (latest/abundant) AND local language sources (regional relevance), with configurable ratios based on content type.

### **Strategy Overview**

**Challenge:**
- English sources: 10-50√ó more abundant, 1-2 weeks earlier, better quality
- Local sources: Essential for law changes, regional news, local business

**Solution:** Smart ratio-based approach

| Content Type | English | Local | Use Cases |
|--------------|---------|-------|-----------|
| **Global/Tech** | 90% | 10% | AI, SaaS, Cloud, Global PropTech trends |
| **Industry** ‚≠ê DEFAULT | 70% | 30% | Real Estate, FinTech, E-commerce, Marketing |
| **National** | 40% | 60% | Law changes, regulations, national business |
| **Hyper-Local** | 20% | 80% | City news, local events, municipal data |

### **Recommended Default: 70/30 Split**

**Why 70/30 works best:**
- ‚úÖ **70% English**: Latest trends, comprehensive research, lower cost
- ‚úÖ **30% Local**: Law changes, regional market dynamics, local nuances
- ‚úÖ **Best Balance**: Global insights + local relevance
- ‚úÖ **Cost-Effective**: Stays within $0.10/article budget
- ‚úÖ **Flexible**: Adjustable per vertical/use case

### **Real-World Examples**

**Example 1: German PropTech Company (70/30)**
```
Topics Discovered:
- 70% English: "AI property valuation 2025", "Smart building IoT trends"
- 30% German: "Mietpreisbremse update", "Berlin Wohnungsmarkt Q4"

Article Output (German):
- Synthesized from 70% English + 30% German sources
- Written entirely in German
- Includes both global trends and local market data
```

**Example 2: German Law Firm (40/60)**
```
Topics Discovered:
- 40% English: "Legal AI automation", "Contract tech trends"
- 60% German: "DSGVO √Ñnderungen 2025", "BGH Urteil Arbeitsrecht"

Article Output (German):
- Heavy focus on German law and regulations
- Global legal tech context from English sources
```

### **Implementation**

**File:** `src/orchestrator/hybrid_research_orchestrator.py`

```python
async def discover_topics_from_collectors(
    self,
    consolidated_keywords: List[str],
    consolidated_tags: List[str],
    max_topics_per_collector: int = 10,
    domain: str = "General",
    vertical: str = "Research",
    market: str = "US",
    language: str = "en",
    english_ratio: float = 0.70,  # NEW: Configurable ratio (default: 70%)
) -> Dict:
    """
    Adaptive multilingual topic discovery with configurable ratios.

    Args:
        language: Target language for content (de, fr, es, etc.)
        english_ratio: Ratio of English sources (0.0-1.0, default: 0.70)
                      0.90 = Global/tech topics
                      0.70 = Industry topics (RECOMMENDED DEFAULT)
                      0.40 = National/regulated topics
                      0.20 = Hyper-local topics

    Strategy:
        - Discover topics from English sources (english_ratio)
        - Discover topics from local sources (1 - english_ratio)
        - Merge and prioritize by freshness + relevance
        - Research using BOTH language sources
        - Synthesize content in target language
    """

    # Calculate topic limits
    english_limit = int(max_topics_per_collector * english_ratio)
    local_limit = int(max_topics_per_collector * (1 - english_ratio))

    # Discover from English sources
    english_topics = await self._discover_in_language(
        keywords=consolidated_keywords,
        tags=consolidated_tags,
        language="en",
        max_topics=english_limit
    )

    # Discover from local language sources
    local_topics = []
    if language != "en" and local_limit > 0:
        local_topics = await self._discover_in_language(
            keywords=consolidated_keywords,
            tags=consolidated_tags,
            language=language,
            max_topics=local_limit
        )

    # Merge and deduplicate
    all_topics = self._merge_topics(english_topics, local_topics)

    return {
        "discovered_topics": all_topics,
        "english_count": len(english_topics),
        "local_count": len(local_topics),
        "ratio": f"{int(english_ratio*100)}/{int((1-english_ratio)*100)}"
    }
```

### **Usage Examples**

**Default (70/30) - Most Use Cases**
```python
# Industry vertical: Real Estate, FinTech, etc.
topics = await orchestrator.discover_topics_from_collectors(
    consolidated_keywords=keywords,
    consolidated_tags=tags,
    language="de",
    english_ratio=0.70  # 70% English, 30% German
)
```

**National Focus (40/60) - Law/Regulations**
```python
# Legal, government, national regulations
topics = await orchestrator.discover_topics_from_collectors(
    consolidated_keywords=keywords,
    consolidated_tags=tags,
    language="de",
    vertical="legal",
    english_ratio=0.40  # 40% English, 60% German
)
```

**Global Tech (90/10) - Latest Trends**
```python
# SaaS, AI, Cloud, global tech
topics = await orchestrator.discover_topics_from_collectors(
    consolidated_keywords=keywords,
    consolidated_tags=tags,
    language="de",
    vertical="saas",
    english_ratio=0.90  # 90% English, 10% German
)
```

### **Benefits**

**Content Quality:**
- ‚úÖ Latest global trends (English sources)
- ‚úÖ Local relevance (German sources for regulations, market data)
- ‚úÖ Comprehensive research (best of both worlds)

**Cost:**
- ‚úÖ Stays within $0.10/article budget
- ‚úÖ English sources are free/cheaper
- ‚úÖ Local sources used only where needed

**Writer Experience:**
- ‚úÖ Access to both English and local sources
- ‚úÖ Can fact-check against original sources
- ‚úÖ Better context for article writing

**Flexibility:**
- ‚úÖ Adjustable per vertical/use case
- ‚úÖ Can tune based on client feedback
- ‚úÖ Future-proof for new markets

---

**Last Updated:** 2025-11-16 (Phase B Integration + Multilingual Strategy Complete)
**Status:** Planning complete, ready for Phase 1 implementation
