# FastAPI Backend Migration Plan

**Project:** Content Creator - Production Architecture Upgrade
**Version:** 1.0
**Status:** Planning Phase
**Date:** 2025-11-23
**Estimated Duration:** 8-12 weeks

---

## ðŸŽ¯ Executive Summary

Migrate from Streamlit monolith to production-grade architecture with:
- **FastAPI** REST API backend (strict type safety, async-first)
- **Postgres** database (scalable, ACID-compliant, full-text search)
- **100% TDD** with 100% test coverage (pytest + coverage.py)
- **CI/CD** via GitHub Actions (automated testing, deployment)
- **Docker** containerization (reproducible, portable)
- **React** frontend (Phase 2 - separate plan)

**Migration Strategy:** Incremental (strangler fig pattern) - zero downtime, gradual rollout

---

## ðŸ“Š Current State Analysis

### Architecture (Monolithic Streamlit)

```
streamlit_app.py
    â†“
â”œâ”€ UI Pages (Streamlit)
â”‚   â”œâ”€ Dashboard
â”‚   â”œâ”€ Generate
â”‚   â”œâ”€ Topic Research
â”‚   â””â”€ Settings
â”‚
â”œâ”€ Agents (Business Logic)
â”‚   â”œâ”€ CompetitorResearchAgent
â”‚   â”œâ”€ KeywordResearchAgent
â”‚   â”œâ”€ ContentPipeline
â”‚   â””â”€ HybridResearchOrchestrator
â”‚
â”œâ”€ Collectors (Data Sources)
â”‚   â”œâ”€ RSSCollector
â”‚   â”œâ”€ RedditCollector
â”‚   â”œâ”€ TrendsCollector
â”‚   â””â”€ AutocompleteCollector
â”‚
â”œâ”€ Research (Deep Research)
â”‚   â”œâ”€ DeepResearcher (multi-backend)
â”‚   â”œâ”€ ContentSynthesizer
â”‚   â””â”€ MultiStageReranker
â”‚
â”œâ”€ Processors (Data Processing)
â”‚   â”œâ”€ Deduplicator
â”‚   â”œâ”€ TopicClusterer
â”‚   â””â”€ EntityExtractor
â”‚
â”œâ”€ Database (SQLite)
â”‚   â””â”€ sqlite_manager.py
â”‚
â””â”€ Notion Integration
    â””â”€ TopicsSync
```

**Problems:**
1. âŒ **Tight coupling**: UI directly calls business logic
2. âŒ **No API layer**: Can't integrate with external systems
3. âŒ **SQLite limitations**: Single-writer, no concurrent access
4. âŒ **No type safety at boundaries**: Runtime errors slip through
5. âŒ **Manual testing**: No CI/CD, deployment is manual
6. âŒ **Monolithic**: Can't scale components independently

### Current Tech Stack

| Component | Technology | Status |
|-----------|-----------|--------|
| **UI** | Streamlit | âš ï¸ Needs replacement |
| **Backend** | Python functions | âš ï¸ Needs API layer |
| **Database** | SQLite | âš ï¸ Needs Postgres |
| **Models** | Pydantic 2.9.1 | âœ… Keep (upgrade to latest) |
| **Testing** | pytest 8.3.3 | âœ… Keep (add coverage) |
| **Queue** | Huey + SQLite | âš ï¸ Upgrade to Huey + Redis |
| **Logging** | structlog | âœ… Keep |
| **CI/CD** | None | âŒ Needs GitHub Actions |
| **Containerization** | None | âŒ Needs Docker |

---

## ðŸ—ï¸ Target Architecture

### Modern 3-Tier Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FRONTEND (Phase 2)                                     â”‚
â”‚  React + TypeScript + Vite                              â”‚
â”‚  - Dashboard, Topic Discovery, Content Browser          â”‚
â”‚  - Real-time updates (WebSockets)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ HTTP/REST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API LAYER (FastAPI)                                    â”‚
â”‚  â”œâ”€ Routes (HTTP handlers)                              â”‚
â”‚  â”œâ”€ Dependencies (auth, DB sessions, rate limits)       â”‚
â”‚  â”œâ”€ Request/Response Models (Pydantic)                  â”‚
â”‚  â””â”€ Middleware (CORS, logging, error handling)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SERVICE LAYER (Business Logic)                         â”‚
â”‚  â”œâ”€ TopicService (discovery, research, validation)      â”‚
â”‚  â”œâ”€ ContentService (generation, synthesis)              â”‚
â”‚  â”œâ”€ CompetitorService (research, analysis)              â”‚
â”‚  â”œâ”€ KeywordService (research, scoring)                  â”‚
â”‚  â””â”€ CollectionService (RSS, Reddit, Trends)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATA ACCESS LAYER (Repositories)                       â”‚
â”‚  â”œâ”€ TopicRepository (CRUD, queries)                     â”‚
â”‚  â”œâ”€ DocumentRepository (storage, search)                â”‚
â”‚  â”œâ”€ CollectionRepository (feed management)              â”‚
â”‚  â””â”€ CacheRepository (Redis for hot data)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATABASE LAYER                                         â”‚
â”‚  â”œâ”€ Postgres (primary data store)                       â”‚
â”‚  â”‚   â”œâ”€ Topics (id, title, status, metadata)            â”‚
â”‚  â”‚   â”œâ”€ Documents (raw collected content)               â”‚
â”‚  â”‚   â”œâ”€ Collections (feed configs, schedules)           â”‚
â”‚  â”‚   â””â”€ FTS (full-text search with tsvector)            â”‚
â”‚  â”œâ”€ Redis (cache, queue, sessions)                      â”‚
â”‚  â””â”€ S3/MinIO (image storage - optional)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BACKGROUND WORKERS (Celery/Huey)                       â”‚
â”‚  â”œâ”€ CollectionWorker (RSS/Reddit polling)               â”‚
â”‚  â”œâ”€ ResearchWorker (deep research jobs)                 â”‚
â”‚  â”œâ”€ SynthesisWorker (content generation)                â”‚
â”‚  â””â”€ NotionSyncWorker (bidirectional sync)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack (Target)

| Layer | Technology | Rationale |
|-------|-----------|-----------|
| **API Framework** | FastAPI 0.115+ | Async-first, OpenAPI, Pydantic native |
| **ASGI Server** | Uvicorn | High performance, WebSocket support |
| **Database** | PostgreSQL 16+ | ACID, full-text search, JSONB, pgvector |
| **ORM** | SQLAlchemy 2.0+ | Type-safe, async support, migrations |
| **Migrations** | Alembic | Schema versioning, rollback support |
| **Cache/Queue** | Redis 7+ | In-memory speed, pub/sub, Celery backend |
| **Task Queue** | Celery | Distributed workers, retry logic, monitoring |
| **Validation** | Pydantic 2.10+ | Runtime validation, strict types |
| **Testing** | pytest + pytest-asyncio | Async support, fixtures, parametrize |
| **Coverage** | coverage.py + pytest-cov | 100% target, branch coverage |
| **Mocking** | pytest-mock + responses | HTTP mocking, API stubbing |
| **CI/CD** | GitHub Actions | Free for public repos, matrix builds |
| **Linting** | Ruff + mypy | Fast linting, strict type checking |
| **Formatting** | Ruff | Black-compatible, fast |
| **Containerization** | Docker + Docker Compose | Multi-stage builds, dev/prod parity |
| **Monitoring** | Prometheus + Grafana | Metrics, dashboards, alerts |
| **Logging** | structlog + JSON | Structured, searchable, ELK-ready |

---

## ðŸ—„ï¸ Database Migration Strategy

### Postgres Schema Design

```sql
-- Enable extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";    -- Fuzzy search
CREATE EXTENSION IF NOT EXISTS "pgvector";   -- Vector similarity (future)

-- Topics (primary entity)
CREATE TABLE topics (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    title VARCHAR(500) NOT NULL,
    description TEXT,

    -- Discovery metadata
    source VARCHAR(50) NOT NULL,  -- rss, reddit, trends, autocomplete
    source_url TEXT,
    discovered_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    -- Classification
    domain VARCHAR(100) NOT NULL,
    market VARCHAR(10) NOT NULL,
    language VARCHAR(10) NOT NULL,
    intent VARCHAR(50),  -- informational, commercial, transactional

    -- Scoring
    engagement_score INTEGER DEFAULT 0,
    trending_score NUMERIC(5,2) DEFAULT 0.0,
    priority INTEGER DEFAULT 5 CHECK (priority BETWEEN 1 AND 10),

    -- Status
    status VARCHAR(50) NOT NULL DEFAULT 'discovered',

    -- Research results (JSONB for flexibility)
    research_report TEXT,
    citations JSONB DEFAULT '[]',
    competitors JSONB DEFAULT '[]',
    content_gaps JSONB DEFAULT '[]',
    keywords JSONB DEFAULT '{}',

    -- Content metadata
    word_count INTEGER,
    content_score NUMERIC(5,2),

    -- Images
    hero_image_url TEXT,
    supporting_images JSONB DEFAULT '[]',

    -- Notion sync
    notion_id VARCHAR(100) UNIQUE,
    notion_synced_at TIMESTAMPTZ,

    -- Timestamps
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    -- Indexes
    CONSTRAINT topics_status_check CHECK (status IN ('discovered', 'validated', 'researched', 'drafted', 'published', 'archived'))
);

-- Indexes for performance
CREATE INDEX idx_topics_status ON topics(status);
CREATE INDEX idx_topics_domain_market ON topics(domain, market);
CREATE INDEX idx_topics_priority_desc ON topics(priority DESC, discovered_at DESC);
CREATE INDEX idx_topics_notion_id ON topics(notion_id) WHERE notion_id IS NOT NULL;

-- Full-text search (Postgres native)
ALTER TABLE topics ADD COLUMN search_vector tsvector;
CREATE INDEX idx_topics_search ON topics USING GIN(search_vector);

-- Auto-update search vector
CREATE OR REPLACE FUNCTION topics_search_vector_update() RETURNS trigger AS $$
BEGIN
    NEW.search_vector :=
        setweight(to_tsvector('english', COALESCE(NEW.title, '')), 'A') ||
        setweight(to_tsvector('english', COALESCE(NEW.description, '')), 'B') ||
        setweight(to_tsvector('english', COALESCE(NEW.research_report, '')), 'C');
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER topics_search_vector_trigger
    BEFORE INSERT OR UPDATE ON topics
    FOR EACH ROW EXECUTE FUNCTION topics_search_vector_update();

-- Auto-update updated_at
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER topics_updated_at_trigger
    BEFORE UPDATE ON topics
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- Documents (collected content)
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),

    -- Content
    title VARCHAR(1000) NOT NULL,
    content TEXT,
    summary TEXT,
    url TEXT NOT NULL,
    canonical_url TEXT,

    -- Source
    source VARCHAR(50) NOT NULL,
    source_metadata JSONB DEFAULT '{}',

    -- Classification
    domain VARCHAR(100) NOT NULL,
    market VARCHAR(10) NOT NULL,
    language VARCHAR(10) NOT NULL,

    -- Deduplication
    content_hash VARCHAR(64) NOT NULL,  -- SHA256 hash

    -- Metadata
    published_at TIMESTAMPTZ,
    fetched_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    author VARCHAR(500),

    -- Quality
    reliability_score NUMERIC(3,2) DEFAULT 0.5,
    paywall BOOLEAN DEFAULT FALSE,

    -- Processing
    status VARCHAR(50) DEFAULT 'new',
    processed_at TIMESTAMPTZ,

    -- Timestamps
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    CONSTRAINT documents_status_check CHECK (status IN ('new', 'processed', 'rejected'))
);

-- Indexes
CREATE INDEX idx_documents_hash ON documents(content_hash);
CREATE INDEX idx_documents_status ON documents(status);
CREATE INDEX idx_documents_domain_market ON documents(domain, market);
CREATE INDEX idx_documents_published_at_desc ON documents(published_at DESC NULLS LAST);

-- Full-text search for documents
ALTER TABLE documents ADD COLUMN search_vector tsvector;
CREATE INDEX idx_documents_search ON documents USING GIN(search_vector);

CREATE TRIGGER documents_search_vector_trigger
    BEFORE INSERT OR UPDATE ON documents
    FOR EACH ROW EXECUTE FUNCTION topics_search_vector_update();

CREATE TRIGGER documents_updated_at_trigger
    BEFORE UPDATE ON documents
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- Collections (feed management)
CREATE TABLE collections (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),

    -- Feed info
    name VARCHAR(200) NOT NULL,
    type VARCHAR(50) NOT NULL,  -- rss, reddit, twitter
    url TEXT NOT NULL,

    -- Configuration
    domain VARCHAR(100) NOT NULL,
    market VARCHAR(10) NOT NULL,
    enabled BOOLEAN DEFAULT TRUE,

    -- Polling
    poll_interval_minutes INTEGER DEFAULT 60,
    last_polled_at TIMESTAMPTZ,
    next_poll_at TIMESTAMPTZ,

    -- Health
    success_count INTEGER DEFAULT 0,
    failure_count INTEGER DEFAULT 0,
    last_error TEXT,

    -- ETag/Last-Modified (HTTP caching)
    etag VARCHAR(200),
    last_modified TIMESTAMPTZ,

    -- Timestamps
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    UNIQUE(type, url, domain, market)
);

-- Indexes
CREATE INDEX idx_collections_enabled ON collections(enabled, next_poll_at) WHERE enabled = TRUE;
CREATE INDEX idx_collections_domain_market ON collections(domain, market);

CREATE TRIGGER collections_updated_at_trigger
    BEFORE UPDATE ON collections
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
```

### Migration Approach

**Option 1: Dual-Write Pattern** (RECOMMENDED)
1. Keep SQLite for reads (existing system)
2. Write to both SQLite + Postgres (new code)
3. Verify data consistency (monitoring)
4. Switch reads to Postgres (feature flag)
5. Retire SQLite

**Option 2: Snapshot + Streaming**
1. Export SQLite to Postgres (one-time)
2. Set up change data capture (CDC)
3. Stream changes to Postgres
4. Cutover when caught up

**Option 3: Big Bang**
1. Freeze writes
2. Export full database
3. Import to Postgres
4. Switch over
5. Resume writes

**Recommendation**: **Option 1** (dual-write) for zero downtime

---

## ðŸ”Œ API Design

### RESTful Endpoints

#### Topics API

```
POST   /api/v1/topics                    # Create topic (manual entry)
GET    /api/v1/topics                    # List topics (pagination, filters)
GET    /api/v1/topics/{id}               # Get topic details
PATCH  /api/v1/topics/{id}               # Update topic
DELETE /api/v1/topics/{id}               # Delete topic

POST   /api/v1/topics/discover           # Trigger topic discovery
POST   /api/v1/topics/{id}/research      # Run deep research
POST   /api/v1/topics/{id}/synthesize    # Generate content
POST   /api/v1/topics/{id}/validate      # Validate topic quality

GET    /api/v1/topics/{id}/competitors   # Get competitor analysis
GET    /api/v1/topics/{id}/keywords      # Get keyword research
```

#### Collections API (Feed Management)

```
POST   /api/v1/collections               # Add feed
GET    /api/v1/collections               # List feeds
GET    /api/v1/collections/{id}          # Get feed details
PATCH  /api/v1/collections/{id}          # Update feed config
DELETE /api/v1/collections/{id}          # Remove feed

POST   /api/v1/collections/{id}/poll     # Trigger immediate poll
GET    /api/v1/collections/{id}/health   # Get feed health stats
```

#### Documents API (Collected Content)

```
GET    /api/v1/documents                 # List documents
GET    /api/v1/documents/{id}            # Get document
DELETE /api/v1/documents/{id}            # Delete document

GET    /api/v1/documents/search          # Full-text search
GET    /api/v1/documents/duplicates      # Find near-duplicates
```

#### Research API

```
POST   /api/v1/research/competitor       # Run competitor research
POST   /api/v1/research/keyword          # Run keyword research
POST   /api/v1/research/deep             # Run deep research (gpt-researcher)
```

#### Admin API

```
GET    /api/v1/admin/health              # Health check
GET    /api/v1/admin/metrics             # Prometheus metrics
GET    /api/v1/admin/config              # Get system config
PATCH  /api/v1/admin/config              # Update config (feature flags)
```

### Request/Response Models (Pydantic)

```python
# app/api/models/topic.py
from datetime import datetime
from typing import Optional, List
from uuid import UUID
from pydantic import BaseModel, Field, HttpUrl, ConfigDict

# Request models
class TopicCreateRequest(BaseModel):
    """Request body for creating a topic manually"""
    title: str = Field(..., min_length=3, max_length=500)
    description: Optional[str] = None
    source: str = "manual"
    domain: str
    market: str
    language: str
    priority: int = Field(default=5, ge=1, le=10)

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "title": "PropTech trends in Germany 2025",
                "description": "Analysis of emerging PropTech technologies",
                "domain": "proptech",
                "market": "de",
                "language": "de",
                "priority": 8
            }
        }
    )

class TopicUpdateRequest(BaseModel):
    """Request body for updating a topic"""
    title: Optional[str] = Field(None, min_length=3, max_length=500)
    description: Optional[str] = None
    priority: Optional[int] = Field(None, ge=1, le=10)
    status: Optional[str] = None

    model_config = ConfigDict(extra='forbid')  # Strict: reject unknown fields

class TopicListRequest(BaseModel):
    """Query parameters for listing topics"""
    page: int = Field(default=1, ge=1)
    page_size: int = Field(default=20, ge=1, le=100)
    domain: Optional[str] = None
    market: Optional[str] = None
    status: Optional[str] = None
    min_priority: Optional[int] = Field(None, ge=1, le=10)
    search: Optional[str] = None  # Full-text search

# Response models
class TopicResponse(BaseModel):
    """Topic response (full details)"""
    id: UUID
    title: str
    description: Optional[str]

    source: str
    source_url: Optional[HttpUrl]
    discovered_at: datetime

    domain: str
    market: str
    language: str
    intent: Optional[str]

    engagement_score: int
    trending_score: float
    priority: int
    status: str

    research_report: Optional[str]
    citations: List[str]
    competitors: List[dict]
    content_gaps: List[str]
    keywords: dict

    word_count: Optional[int]
    content_score: Optional[float]

    hero_image_url: Optional[HttpUrl]
    supporting_images: List[dict]

    notion_id: Optional[str]
    notion_synced_at: Optional[datetime]

    created_at: datetime
    updated_at: datetime

    model_config = ConfigDict(from_attributes=True)  # ORM mode

class TopicListResponse(BaseModel):
    """Paginated topic list"""
    items: List[TopicResponse]
    total: int
    page: int
    page_size: int
    pages: int

class TopicSummaryResponse(BaseModel):
    """Topic summary (list view)"""
    id: UUID
    title: str
    status: str
    priority: int
    domain: str
    market: str
    discovered_at: datetime
```

### Error Handling

```python
# app/api/errors.py
from fastapi import HTTPException, status
from pydantic import BaseModel

class ErrorDetail(BaseModel):
    """Standardized error response"""
    error_code: str
    message: str
    details: dict = {}

class APIError(HTTPException):
    """Base API error"""
    def __init__(self, error_code: str, message: str, details: dict = {}, status_code: int = 400):
        super().__init__(
            status_code=status_code,
            detail=ErrorDetail(
                error_code=error_code,
                message=message,
                details=details
            ).model_dump()
        )

# Specific errors
class TopicNotFoundError(APIError):
    def __init__(self, topic_id: str):
        super().__init__(
            error_code="TOPIC_NOT_FOUND",
            message=f"Topic {topic_id} not found",
            status_code=status.HTTP_404_NOT_FOUND
        )

class TopicValidationError(APIError):
    def __init__(self, errors: dict):
        super().__init__(
            error_code="TOPIC_VALIDATION_FAILED",
            message="Topic validation failed",
            details={"errors": errors},
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY
        )
```

---

## ðŸ§ª Testing Strategy (100% Coverage)

### Test Pyramid

```
         /\
        /  \       E2E Tests (5%)
       /____\      - Full workflow tests
      /      \     - API integration tests
     /        \
    /__________\   Integration Tests (15%)
   /            \  - Database integration
  /              \ - External API mocking
 /________________\
/                  \
|  Unit Tests (80%) |
|  - Business logic |
|  - Validators     |
|  - Utilities      |
```

### Testing Layers

#### 1. Unit Tests (80% of tests)

**Target**: Individual functions, classes, methods

```python
# tests/unit/services/test_topic_service.py
import pytest
from uuid import UUID
from app.services.topic_service import TopicService
from app.models.topic import TopicCreateRequest
from unittest.mock import Mock, AsyncMock

@pytest.fixture
def topic_service():
    """Create topic service with mocked dependencies"""
    repo = Mock()
    cache = Mock()
    return TopicService(repository=repo, cache=cache)

@pytest.mark.asyncio
async def test_create_topic_success(topic_service):
    """Test successful topic creation"""
    # Arrange
    request = TopicCreateRequest(
        title="Test Topic",
        domain="proptech",
        market="de",
        language="de"
    )
    topic_service.repository.create = AsyncMock(return_value={"id": UUID("...")})

    # Act
    result = await topic_service.create_topic(request)

    # Assert
    assert result["title"] == "Test Topic"
    topic_service.repository.create.assert_called_once()

@pytest.mark.asyncio
async def test_create_topic_duplicate_title(topic_service):
    """Test topic creation with duplicate title"""
    # Arrange
    request = TopicCreateRequest(title="Duplicate", ...)
    topic_service.repository.exists_by_title = AsyncMock(return_value=True)

    # Act & Assert
    with pytest.raises(TopicValidationError) as exc:
        await topic_service.create_topic(request)

    assert "duplicate" in str(exc.value).lower()
```

#### 2. Integration Tests (15% of tests)

**Target**: Database, external APIs, service interactions

```python
# tests/integration/test_topic_repository.py
import pytest
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from app.db.repositories.topic_repository import TopicRepository
from app.db.models import Topic

@pytest.fixture
async def db_session():
    """Create test database session"""
    engine = create_async_engine("postgresql+asyncpg://test:test@localhost:5433/test_db")
    # Create tables
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    async with AsyncSession(engine) as session:
        yield session

    # Cleanup
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)

@pytest.mark.asyncio
async def test_topic_repository_create_and_find(db_session):
    """Test topic creation and retrieval"""
    # Arrange
    repo = TopicRepository(db_session)
    topic_data = {
        "title": "Integration Test Topic",
        "domain": "proptech",
        "market": "de",
        "language": "de"
    }

    # Act
    created = await repo.create(topic_data)
    found = await repo.get_by_id(created.id)

    # Assert
    assert found is not None
    assert found.title == topic_data["title"]
    assert found.status == "discovered"

@pytest.mark.asyncio
async def test_topic_full_text_search(db_session):
    """Test Postgres full-text search"""
    repo = TopicRepository(db_session)

    # Create test topics
    await repo.create({"title": "PropTech trends in Germany", ...})
    await repo.create({"title": "Fashion e-commerce in France", ...})

    # Search
    results = await repo.search("PropTech Germany")

    # Assert
    assert len(results) == 1
    assert "proptech" in results[0].title.lower()
```

#### 3. E2E Tests (5% of tests)

**Target**: Full API workflows

```python
# tests/e2e/test_topic_workflow.py
import pytest
from httpx import AsyncClient
from app.main import app

@pytest.mark.asyncio
async def test_topic_discovery_workflow():
    """Test complete topic discovery workflow"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        # 1. Create topic
        create_response = await client.post("/api/v1/topics", json={
            "title": "E2E Test Topic",
            "domain": "proptech",
            "market": "de",
            "language": "de"
        })
        assert create_response.status_code == 201
        topic_id = create_response.json()["id"]

        # 2. Run competitor research
        competitor_response = await client.post(
            f"/api/v1/topics/{topic_id}/research/competitor"
        )
        assert competitor_response.status_code == 200

        # 3. Run keyword research
        keyword_response = await client.post(
            f"/api/v1/topics/{topic_id}/research/keyword"
        )
        assert keyword_response.status_code == 200

        # 4. Verify topic updated
        get_response = await client.get(f"/api/v1/topics/{topic_id}")
        topic = get_response.json()
        assert len(topic["competitors"]) > 0
        assert len(topic["keywords"]) > 0
        assert topic["status"] == "researched"
```

### Coverage Requirements

```bash
# pytest.ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts =
    --cov=app
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=100
    --strict-markers
    --asyncio-mode=auto

markers =
    unit: Unit tests (fast, no external dependencies)
    integration: Integration tests (database, external APIs)
    e2e: End-to-end tests (full workflows)
    slow: Slow tests (run separately in CI)
```

**Coverage Targets**:
- Overall: **100%** (strict)
- API routes: 100% (all endpoints covered)
- Services: 100% (all business logic)
- Repositories: 100% (all database operations)
- Models: 100% (all validators)

**Exclusions**:
- `__init__.py` files (imports only)
- Configuration files
- Migrations

---

## ðŸš€ CI/CD Pipeline (GitHub Actions)

### Workflow Structure

```yaml
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

env:
  PYTHON_VERSION: "3.12"
  POSTGRES_VERSION: "16"

jobs:
  lint:
    name: Lint & Type Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}

      - name: Install dependencies
        run: |
          pip install ruff mypy
          pip install -r requirements-dev.txt

      - name: Run Ruff (linting)
        run: ruff check . --output-format=github

      - name: Run Ruff (formatting)
        run: ruff format --check .

      - name: Run mypy (type checking)
        run: mypy app tests --strict

  test:
    name: Test (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11", "3.12"]

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('**/requirements*.txt') }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run migrations
        env:
          DATABASE_URL: postgresql+asyncpg://test:test@localhost:5432/test_db
        run: alembic upgrade head

      - name: Run tests
        env:
          DATABASE_URL: postgresql+asyncpg://test:test@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379/0
        run: |
          pytest \
            --cov=app \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-fail-under=100 \
            -v

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  security:
    name: Security Scan
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run Trivy (vulnerability scanner)
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Run Bandit (Python security linter)
        run: |
          pip install bandit
          bandit -r app -f json -o bandit-report.json

  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [lint, test, security]
    if: github.event_name == 'push'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository }}
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [build]
    if: github.ref == 'refs/heads/develop'
    steps:
      - name: Deploy to staging
        run: |
          # Add deployment script here
          echo "Deploy to staging environment"
```

### Pre-commit Hooks

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.7.2
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.11.2
    hooks:
      - id: mypy
        additional_dependencies: [pydantic, sqlalchemy]

  - repo: https://github.com/gitleaks/gitleaks
    rev: v8.21.2
    hooks:
      - id: gitleaks
```

---

## ðŸ³ Docker Architecture

### Multi-Stage Dockerfile

```dockerfile
# Dockerfile
# Stage 1: Base (dependencies)
FROM python:3.12-slim AS base

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    postgresql-client \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Stage 2: Development
FROM base AS development

# Install dev dependencies
COPY requirements-dev.txt .
RUN pip install --no-cache-dir -r requirements-dev.txt

# Copy source code
COPY . .

# Run as non-root
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

# Stage 3: Testing
FROM development AS testing

# Run tests
RUN pytest --cov=app --cov-fail-under=100

# Stage 4: Production
FROM base AS production

# Copy only necessary files
COPY app /app/app
COPY alembic /app/alembic
COPY alembic.ini /app/

# Run as non-root
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/api/v1/admin/health || exit 1

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### Docker Compose

```yaml
# docker-compose.yml
version: '3.9'

services:
  # PostgreSQL database
  postgres:
    image: postgres:16-alpine
    container_name: content-creator-db
    environment:
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-postgres}
      POSTGRES_DB: ${DB_NAME:-content_creator}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./migrations/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - backend

  # Redis cache/queue
  redis:
    image: redis:7-alpine
    container_name: content-creator-cache
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - backend

  # FastAPI application
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: content-creator-api
    environment:
      DATABASE_URL: postgresql+asyncpg://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@postgres:5432/${DB_NAME:-content_creator}
      REDIS_URL: redis://redis:6379/0
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    volumes:
      - .:/app
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - backend
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  # Celery worker
  worker:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: content-creator-worker
    environment:
      DATABASE_URL: postgresql+asyncpg://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@postgres:5432/${DB_NAME:-content_creator}
      REDIS_URL: redis://redis:6379/0
      CELERY_BROKER_URL: redis://redis:6379/1
      CELERY_RESULT_BACKEND: redis://redis:6379/2
    volumes:
      - .:/app
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - backend
    command: celery -A app.tasks.celery_app worker --loglevel=info

  # Celery beat (scheduler)
  beat:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: content-creator-beat
    environment:
      DATABASE_URL: postgresql+asyncpg://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@postgres:5432/${DB_NAME:-content_creator}
      REDIS_URL: redis://redis:6379/0
      CELERY_BROKER_URL: redis://redis:6379/1
      CELERY_RESULT_BACKEND: redis://redis:6379/2
    volumes:
      - .:/app
    depends_on:
      - redis
    networks:
      - backend
    command: celery -A app.tasks.celery_app beat --loglevel=info

  # Flower (Celery monitoring)
  flower:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: content-creator-flower
    environment:
      CELERY_BROKER_URL: redis://redis:6379/1
      CELERY_RESULT_BACKEND: redis://redis:6379/2
    ports:
      - "5555:5555"
    depends_on:
      - redis
    networks:
      - backend
    command: celery -A app.tasks.celery_app flower --port=5555

volumes:
  postgres_data:
  redis_data:

networks:
  backend:
    driver: bridge
```

---

## ðŸ“‹ Implementation Phases

### Phase 1: Foundation (Weeks 1-2)

**Goals**: Set up infrastructure, database, basic API

**Tasks**:
1. âœ… Project structure setup
   - Initialize FastAPI project
   - Create directory structure (app/, tests/, alembic/)
   - Set up configuration management (pydantic-settings)

2. âœ… Database setup
   - Install PostgreSQL 16
   - Create initial schema (topics, documents, collections)
   - Set up Alembic migrations
   - Create SQLAlchemy 2.0 models (async)

3. âœ… Basic API skeleton
   - FastAPI app with CORS middleware
   - Health check endpoint
   - Dependency injection setup
   - Error handling middleware

4. âœ… Testing infrastructure
   - pytest configuration
   - Test database setup
   - Fixtures for common test data
   - Coverage configuration (100% target)

5. âœ… CI/CD setup
   - GitHub Actions workflow
   - Linting (Ruff)
   - Type checking (mypy --strict)
   - Test execution with coverage

**Deliverables**:
- âœ… Running FastAPI app with `/health` endpoint
- âœ… Postgres database with migrations
- âœ… CI/CD pipeline passing
- âœ… Test coverage >95%

---

### Phase 2: Data Layer (Weeks 3-4)

**Goals**: Implement repositories, database operations

**Tasks**:
1. âœ… Repository pattern
   - BaseRepository (CRUD operations)
   - TopicRepository (with full-text search)
   - DocumentRepository (with deduplication)
   - CollectionRepository (with health tracking)

2. âœ… SQLite â†’ Postgres migration
   - Export existing data
   - Import to Postgres
   - Verify data integrity
   - Update indexes

3. âœ… Redis integration
   - Cache layer for hot data
   - Session storage
   - Rate limiting store

4. âœ… Database testing
   - Integration tests for all repositories
   - Transaction rollback in tests
   - Performance benchmarks

**Deliverables**:
- âœ… All repositories implemented with 100% coverage
- âœ… Data migrated from SQLite to Postgres
- âœ… Redis cache working

---

### Phase 3: Service Layer (Weeks 5-6)

**Goals**: Implement business logic services

**Tasks**:
1. âœ… Topic service
   - CRUD operations
   - Discovery workflows
   - Research orchestration
   - Validation logic

2. âœ… Collection service
   - Feed management
   - Polling logic
   - Health monitoring

3. âœ… Research services
   - CompetitorService (existing agent wrapper)
   - KeywordService (existing agent wrapper)
   - DeepResearchService (gpt-researcher wrapper)

4. âœ… Content service
   - ContentPipeline wrapper
   - Image generation
   - Notion sync

**Deliverables**:
- âœ… All services implemented
- âœ… 100% test coverage
- âœ… Service integration tests passing

---

### Phase 4: API Layer (Weeks 7-8)

**Goals**: Implement REST API endpoints

**Tasks**:
1. âœ… Topics API
   - CRUD endpoints
   - List with pagination
   - Full-text search
   - Research triggers

2. âœ… Collections API
   - Feed management
   - Polling triggers
   - Health endpoints

3. âœ… Documents API
   - List/search endpoints
   - Deduplication API

4. âœ… Admin API
   - Health checks
   - Metrics (Prometheus)
   - Configuration

5. âœ… API documentation
   - OpenAPI/Swagger UI
   - Redoc
   - Example requests

**Deliverables**:
- âœ… All API endpoints implemented
- âœ… 100% test coverage
- âœ… OpenAPI documentation complete

---

### Phase 5: Background Workers (Weeks 9-10)

**Goals**: Implement async task processing

**Tasks**:
1. âœ… Celery setup
   - Redis broker
   - Worker configuration
   - Beat scheduler

2. âœ… Collection tasks
   - RSS polling
   - Reddit monitoring
   - Trends collection

3. âœ… Research tasks
   - Deep research jobs
   - Content synthesis
   - Image generation

4. âœ… Notion sync tasks
   - Bidirectional sync
   - Conflict resolution

**Deliverables**:
- âœ… All workers implemented
- âœ… Task monitoring (Flower)
- âœ… Retry logic working

---

### Phase 6: Deployment (Weeks 11-12)

**Goals**: Production deployment

**Tasks**:
1. âœ… Docker optimization
   - Multi-stage builds
   - Image size <200MB
   - Security scanning

2. âœ… Production configuration
   - Environment variables
   - Secrets management
   - Logging configuration

3. âœ… Monitoring setup
   - Prometheus metrics
   - Grafana dashboards
   - Alerting rules

4. âœ… Documentation
   - API documentation
   - Deployment guide
   - Runbook

**Deliverables**:
- âœ… Production-ready Docker images
- âœ… Monitoring dashboards
- âœ… Complete documentation

---

## ðŸ“ Project Structure (Target)

```
content-creator-api/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                 # CI/CD pipeline
â”‚       â””â”€â”€ deploy.yml             # Deployment workflow
â”‚
â”œâ”€â”€ alembic/                       # Database migrations
â”‚   â”œâ”€â”€ versions/
â”‚   â””â”€â”€ env.py
â”‚
â”œâ”€â”€ app/                           # Main application
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # FastAPI app entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                       # API layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dependencies.py        # Dependency injection
â”‚   â”‚   â”œâ”€â”€ errors.py              # Error handlers
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ v1/                    # API v1
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ router.py          # Main router
â”‚   â”‚   â”‚   â”œâ”€â”€ topics.py          # Topics endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ collections.py     # Collections endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py       # Documents endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ research.py        # Research endpoints
â”‚   â”‚   â”‚   â””â”€â”€ admin.py           # Admin endpoints
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ models/                # Request/response models
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ topic.py           # Topic API models
â”‚   â”‚       â”œâ”€â”€ collection.py      # Collection API models
â”‚   â”‚       â””â”€â”€ common.py          # Shared models
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                      # Core configuration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py              # App settings (Pydantic)
â”‚   â”‚   â”œâ”€â”€ security.py            # Auth/security
â”‚   â”‚   â””â”€â”€ logging.py             # Structured logging
â”‚   â”‚
â”‚   â”œâ”€â”€ db/                        # Database layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ session.py             # Async session factory
â”‚   â”‚   â”œâ”€â”€ models/                # SQLAlchemy models
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py            # Base model
â”‚   â”‚   â”‚   â”œâ”€â”€ topic.py           # Topic model
â”‚   â”‚   â”‚   â”œâ”€â”€ document.py        # Document model
â”‚   â”‚   â”‚   â””â”€â”€ collection.py      # Collection model
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ repositories/          # Repository pattern
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ base.py            # BaseRepository
â”‚   â”‚       â”œâ”€â”€ topic.py           # TopicRepository
â”‚   â”‚       â”œâ”€â”€ document.py        # DocumentRepository
â”‚   â”‚       â””â”€â”€ collection.py      # CollectionRepository
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                  # Business logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ topic.py               # TopicService
â”‚   â”‚   â”œâ”€â”€ collection.py          # CollectionService
â”‚   â”‚   â”œâ”€â”€ research.py            # ResearchService
â”‚   â”‚   â””â”€â”€ content.py             # ContentService
â”‚   â”‚
â”‚   â”œâ”€â”€ tasks/                     # Background tasks (Celery)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ celery_app.py          # Celery configuration
â”‚   â”‚   â”œâ”€â”€ collection.py          # Collection tasks
â”‚   â”‚   â”œâ”€â”€ research.py            # Research tasks
â”‚   â”‚   â””â”€â”€ notion.py              # Notion sync tasks
â”‚   â”‚
â”‚   â””â”€â”€ utils/                     # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cache.py               # Redis cache helpers
â”‚       â””â”€â”€ validators.py          # Custom validators
â”‚
â”œâ”€â”€ tests/                         # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                # Pytest fixtures
â”‚   â”‚
â”‚   â”œâ”€â”€ unit/                      # Unit tests (80%)
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚
â”‚   â”œâ”€â”€ integration/               # Integration tests (15%)
â”‚   â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â””â”€â”€ external/
â”‚   â”‚
â”‚   â””â”€â”€ e2e/                       # E2E tests (5%)
â”‚       â””â”€â”€ test_topic_workflow.py
â”‚
â”œâ”€â”€ migrations/                    # SQL migrations
â”‚   â””â”€â”€ init.sql
â”‚
â”œâ”€â”€ docker/                        # Docker configs
â”‚   â”œâ”€â”€ Dockerfile.dev
â”‚   â””â”€â”€ Dockerfile.prod
â”‚
â”œâ”€â”€ .env.example                   # Environment template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ alembic.ini                    # Alembic config
â”œâ”€â”€ docker-compose.yml             # Local development
â”œâ”€â”€ pyproject.toml                 # Project metadata
â”œâ”€â”€ pytest.ini                     # Pytest config
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt               # Production deps
â””â”€â”€ requirements-dev.txt           # Dev deps
```

---

## âœ… Success Criteria

### Technical

- [x] **100% test coverage** - All code covered by tests
- [ ] **100% type safety** - mypy --strict passing
- [ ] **Zero security vulnerabilities** - Trivy + Bandit clean
- [ ] **API response time <100ms** - p95 latency
- [ ] **Database queries <50ms** - p95 query time
- [ ] **CI/CD pipeline <5min** - Fast feedback loop

### Functional

- [ ] **All existing features working** - No regressions
- [ ] **API documentation complete** - OpenAPI + examples
- [ ] **Deployment automated** - One-command deploy
- [ ] **Monitoring in place** - Metrics + alerts
- [ ] **Zero downtime migration** - Gradual rollout

---

## ðŸŽ¯ Next Steps

1. **Review this plan** - Gather feedback, adjust estimates
2. **Set up repo structure** - Create branches, directories
3. **Phase 1 kickoff** - Start with database schema
4. **Weekly check-ins** - Progress reviews, blockers

---

**Questions? Comments?** Let's discuss before we start implementation!
