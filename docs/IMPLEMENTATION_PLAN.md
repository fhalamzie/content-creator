# Universal Topic Research Agent - Implementation Plan

**Version**: 2.0
**Status**: âœ… Architecture Validated (Multi-Model Consensus)
**Last Updated**: 2025-11-04
**Key Changes**:
- Optimized SQLite (validated sufficient for 500 feeds/day)
- SerpAPI + Gemini CLI hybrid (validated $0/month cost)
- Simplified 2-stage feed discovery (validated reliability)
- TF-IDF clustering without embeddings (validated for MVP)
- No per-language clustering needed (per-config isolation)

**Consensus Score**: 9.5/10 (GPT-5, DeepSeek, Gemini 2.5 Pro)

---

## 1. Executive Summary

### What We're Building
A **configuration-driven universal agent** that discovers SEO-optimized topics for ANY domain, market, and language by leveraging existing open-source libraries.

**One agent, infinite niches**:
```python
# German Proptech SaaS
agent = UniversalTopicAgent(domain="SaaS", market="Germany", language="de", vertical="Proptech")

# French Fashion E-commerce
agent = UniversalTopicAgent(domain="E-commerce", market="France", language="fr", vertical="Fashion")
```

### Core Value Proposition
- **100% Automated Discovery** - From 2-3 seed keywords â†’ 80-150 feeds discovered
- **Deep Research with Citations** - gpt-researcher generates 5-6 page sourced reports
- **âœ… VALIDATED $0/month Stack** - SerpAPI (90 free/month) + Gemini CLI (unlimited)
- **Universal Configuration** - ANY domain/market/language (one config file)
- **Production-Ready Architecture** - Validated by GPT-5, DeepSeek, Gemini 2.5 Pro

### Key Architectural Decisions (âœ… Multi-Model Validated)

| Decision | Validation | Risk Mitigation |
|----------|-----------|-----------------|
| **Optimized SQLite** (not Postgres) | âœ… GPT-5: Sufficient for 500 feeds/day | WAL checkpoint monitoring, migrate at >100K docs |
| **SerpAPI + Gemini CLI** hybrid | âœ… All models: Cost-effective | 3 req/day hard cap, 30-day cache, circuit breaker |
| **2-Stage Feed Discovery** | âœ… GPT-5: Simpler & reliable | OPML seeds + periodic wildcard sampling |
| **TF-IDF (skip embeddings)** | âœ… GPT-5: Can ship without | Per-config isolation (no language mixing) |
| **Huey + SQLite** | âœ… GPT-5: Adequate | Single writer, DLQ, retry logic |
| **LLM-First NLP** (qwen-turbo) | âš ï¸ All models: Monitor costs | Aggressive caching, Pydantic validation |

---

## 2. System Architecture

### Data Flow (âœ… Validated Architecture)
```
User Config (domain/market/language) [Per-config isolation = no language mixing]
    â†“
2-Stage Feed Discovery [Simpler than original 4-stage]
    â”œâ”€ Stage 1: OPML Seeds (curated) + Gemini CLI expansion (FREE)
    â””â”€ Stage 2: SerpAPI (3/day, cached 30d) + feedfinder2
    â†“
Parallel Collection (RSS with trafilatura, Reddit, Trends, Autocomplete)
    â”œâ”€ Feed health tracking (adaptive polling)
    â”œâ”€ Per-host rate limiting + robots.txt
    â””â”€ ETag/Last-Modified (conditional GET)
    â†“
Unified Document Model + Language Detection (qwen-turbo, cached 30d)
    â†“
Deduplication (MinHash/LSH + canonical URL normalization)
    â†“
Simple TF-IDF Clustering [No per-language separation needed]
    â”œâ”€ Per-config = already single language (~95%)
    â””â”€ LLM labeling only (cheap, explainable)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5-STAGE ENHANCED PIPELINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                            â”‚
â”‚  1. Competitor Research (Gemini CLI - FREE)                â”‚
â”‚     â†’ Identify content gaps & opportunities                â”‚
â”‚                                                            â”‚
â”‚  2. Keyword Research (Gemini CLI - FREE)                   â”‚
â”‚     â†’ Find high-value SEO keywords                         â”‚
â”‚                                                            â”‚
â”‚  3. Deep Research (gpt-researcher + DuckDuckGo)            â”‚
â”‚     â†’ Generate sourced reports with citations              â”‚
â”‚     â†’ Enhanced with competitor gaps + keywords             â”‚
â”‚                                                            â”‚
â”‚  4. Content Optimization                                   â”‚
â”‚     â†’ Apply SEO insights from keyword research             â”‚
â”‚     â†’ Add competitive intelligence metadata                â”‚
â”‚                                                            â”‚
â”‚  5. Scoring & Ranking                                      â”‚
â”‚     â†’ Demand + Opportunity + Fit + Novelty                 â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Notion Sync (Top topics with full metadata)
```

### Technology Stack (âœ… Validated by Consensus)

| Component | Implementation | Validation | Key Config |
|-----------|---------------|------------|------------|
| **Database** | SQLite (WAL) | âœ… GPT-5: Sufficient for 500 feeds/day | `PRAGMA journal_mode=WAL, cache_size=-64000` |
| **Task Queue** | Huey + SQLite | âœ… GPT-5: Adequate for MVP | Single writer, DLQ, retry logic |
| **SERP Discovery** | SerpAPI (100 free/mo) | âœ… All models: Reliable | 3 req/day hard cap, 30-day cache |
| **LLM Expansion** | Gemini CLI (FREE) | âœ… GPT-5: Cost reducer | Retry logic + fallback to basic keywords |
| **Feed Discovery** | feedfinder2 + OPML | âœ… GPT-5: Simpler approach | Curated seeds + periodic wildcard sampling |
| **Content Extraction** | trafilatura | âœ… GPT-5: Clean extraction | Handle summary-only feeds |
| **Clustering** | TF-IDF + HDBSCAN | âœ… GPT-5: Can ship without embeddings | Per-config isolation (no mixing) |
| **LLM Processing** | qwen-turbo (OpenRouter) | âš ï¸ All models: Monitor costs | 30-day cache, Pydantic validation |
| **Deep Research** | gpt-researcher | âœ… Original plan | Citations built-in |
| **Observability** | structlog + metrics | âœ… GPT-5: Day 1 requirement | WAL size, API usage, error rates |

**Migration Triggers** (not needed for MVP):
- SQLite â†’ Postgres: If >100K documents OR >10 concurrent workers OR WAL issues
- Huey + SQLite â†’ Huey + Redis: If >10K tasks/day
- TF-IDF â†’ Embeddings: If cross-lingual similarity needed (Phase 3)

---

## 3. Critical Risks & Mitigations (Multi-Model Consensus)

### ğŸš¨ **Top 5 Production Risks** (Identified by GPT-5 & DeepSeek)

| Risk | Impact | Mitigation | Priority |
|------|--------|------------|----------|
| **1. SQLite WAL Unbounded Growth** | Database read-only, feed processing halts | `wal_autocheckpoint=1000`, WAL size monitoring + alerts, force checkpoint if >10MB | ğŸ”´ HIGH |
| **2. SerpAPI Quota Exhaustion** | 100 req/month limit hit in 24h â†’ 29 days degraded | 3 req/day hard cap, circuit breaker, graceful degradation to cache | ğŸ”´ HIGH |
| **3. Gemini CLI Malformed JSON** | Feed discovery pipeline breaks, garbage keywords | Robust retry (2 attempts), fallback to basic keyword extraction | ğŸŸ¡ MEDIUM |
| **4. LLM API Latency >10s** | Pipeline slowdown, timeout cascades | 30-day aggressive caching, circuit breaker, fallback to cached | ğŸŸ¡ MEDIUM |
| **5. Feed Diversity Echo Chamber** | Missing niche sources, content homogenization | Periodic wildcard sampling (10% of runs), manual quality review | ğŸŸ¢ LOW |

### âœ… **Validated Mitigations** (Implementation Required)

```python
# 1. SQLite WAL Monitoring (DeepSeek fix)
def monitor_wal_health():
    wal_size_mb = get_wal_size()
    if wal_size_mb > 10:
        logger.warning(f"WAL growing: {wal_size_mb}MB")
        conn.execute('PRAGMA wal_checkpoint(TRUNCATE)')
        metrics.increment('wal_checkpoint_forced')

# 2. SerpAPI Circuit Breaker (DeepSeek fix)
class SerpAPIClient:
    daily_limit = 3  # Hard cap: 3/day = 90/month (under 100 free)

    def search(self, query):
        if self.get_daily_usage() >= self.daily_limit:
            logger.warning("SerpAPI daily limit reached")
            return self.get_cached_results(query)  # Graceful degradation

# 3. Gemini CLI Fallback (DeepSeek fix)
def expand_keywords_gemini(seeds, retries=2):
    try:
        return call_gemini_cli(seeds)
    except Exception as e:
        logger.error(f"Gemini expansion failed: {e}")
        return basic_keyword_fallback(seeds)  # Regex-based extraction

# 4. LLM Response Caching (GPT-5 recommendation)
@cache(ttl=30*86400)  # 30-day cache
def llm_process(prompt, model):
    # Deterministic: temperature=0, version-pinned model
    return openrouter.call(prompt, model, temperature=0)

# 5. Feed Discovery Wildcard (DeepSeek fix)
if random.random() < 0.1:  # 10% of discovery runs
    wildcard_feeds = sample_rejected_feeds()
    feeds.extend(wildcard_feeds)
```

---

## 4. Modular Architecture (No Import Circles!)

**Design Principles**:
1. **Layered Architecture** - Clear dependency direction (bottom-up only)
2. **Dependency Injection** - Pass dependencies explicitly (no global state)
3. **Central Logging** - Single logger instance, structured logging everywhere

**Module Layers** (dependencies flow DOWN only):
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  main.py (Entry Point)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  agents/ (Orchestration Layer)          â”‚  â† Uses collectors, processors
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  collectors/ (Data Collection)          â”‚  â† Uses models, utils
â”‚  processors/ (Data Processing)          â”‚  â† Uses models, utils
â”‚  research/ (Deep Research)              â”‚  â† Uses models, utils
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  database/ (Data Access)                â”‚  â† Uses models only
â”‚  tasks/ (Background Jobs)               â”‚  â† Uses agents, database
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  models/ (Data Models)                  â”‚  â† No dependencies
â”‚  utils/ (Utilities)                     â”‚  â† No dependencies
â”‚  config/ (Configuration)                â”‚  â† No dependencies
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Import Rules**:
```python
# âœ… ALLOWED (down the stack)
from src.models.topic import Topic              # models (bottom)
from src.utils.logger import get_logger         # utils (bottom)
from src.database.manager import DatabaseManager # DB uses models
from src.collectors.rss import RSSCollector     # collectors use models

# âŒ FORBIDDEN (up the stack)
# models importing from collectors - NO!
# utils importing from agents - NO!
# database importing from collectors - NO!
```

### Central Logging System

**Single source of truth** - `src/utils/logger.py`:

```python
# src/utils/logger.py
import structlog
import logging
import sys

# Configure once, use everywhere
def setup_logging(log_level: str = "INFO"):
    """
    Central logging configuration

    Usage: Call ONCE in main.py
    """
    logging.basicConfig(
        format="%(message)s",
        stream=sys.stdout,
        level=getattr(logging, log_level.upper())
    )

    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.processors.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.dev.ConsoleRenderer()
        ],
        wrapper_class=structlog.make_filtering_bound_logger(
            getattr(logging, log_level.upper())
        ),
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(),
        cache_logger_on_first_use=True
    )

def get_logger(name: str) -> structlog.BoundLogger:
    """
    Get logger for module

    Usage in ANY module:
        from src.utils.logger import get_logger
        logger = get_logger(__name__)
        logger.info("event", key="value")
    """
    return structlog.get_logger(name)
```

**Usage everywhere**:
```python
# src/collectors/rss_collector.py
from src.utils.logger import get_logger

logger = get_logger(__name__)

class RSSCollector:
    def collect(self):
        logger.info("rss_collection_started", feed_count=len(self.feeds))

        try:
            articles = self._fetch_all()
            logger.info("rss_collection_success",
                       article_count=len(articles),
                       duration=elapsed)
        except Exception as e:
            logger.error("rss_collection_failed",
                        error=str(e),
                        feed=feed_url)
            raise
```

**Structured logging output**:
```json
{"event": "rss_collection_started", "feed_count": 20, "timestamp": "2025-11-03T12:00:00Z", "level": "info"}
{"event": "rss_collection_success", "article_count": 142, "duration": 3.2, "timestamp": "2025-11-03T12:00:03Z", "level": "info"}
```

### Dependency Injection Pattern

**No global state, explicit dependencies**:

```python
# âŒ BAD - Global state
db = DatabaseManager()  # Global!

class RSSCollector:
    def collect(self):
        db.insert(articles)  # Uses global!

# âœ… GOOD - Dependency injection
class RSSCollector:
    def __init__(self, db: DatabaseManager, llm: LLMProcessor):
        self.db = db
        self.llm = llm

    def collect(self):
        self.db.insert(articles)  # Uses injected!

# Compose in main.py
db = DatabaseManager(config.database)
llm = LLMProcessor()
rss_collector = RSSCollector(db=db, llm=llm)
```

---

## 3. Dependencies & Libraries

### Core Stack (pip install)
```bash
# RSS & Feeds (PRIMARY - unlimited, free)
feedparser==6.0.11              # Battle-tested RSS parser
feedfinder2==0.0.4              # Auto-detect RSS links
trafilatura==1.12.1             # Primary article extraction (all languages)
newspaper3k==0.2.8              # Fallback only

# Reddit (Free: 60 requests/min)
praw==7.7.1                     # Official API

# Trends & SEO
pytrends==4.9.2                 # Google Trends (unofficial but free)
advertools==0.14.2              # Sitemap crawling, no API key

# Search & Discovery
duckduckgo-search==6.3.5        # FREE SERP for intelligent feed discovery

# Deep Research
gpt-researcher==0.14.4          # Citations built-in

# Deduplication
datasketch==1.6.4               # MinHash/LSH

# Rate Limiting & Retries
tenacity==8.5.0                 # Exponential backoff
aiolimiter==1.1.0               # Async rate limiting

# Task Queue (MVP: Simple)
huey==2.5.0                     # SQLite-based task queue

# Observability
structlog==24.4.0               # Structured logging

# Content Analysis (Phase 2)
textstat==0.7.3                 # Readability scoring

# LLM (replaces 5GB of NLP dependencies!)
openai==1.54.0                  # OpenRouter client for qwen-turbo

# Database
# sqlite3 (built-in)            # MVP
# psycopg[binary]==3.2.1        # Production
# pgvector==0.3.0               # Vector search

# Notion
notion-client==2.2.1

# Utilities
pydantic==2.9.1
pyyaml==6.0.2
httpx==0.27.0
```

### LLM Strategy (Zero Dependencies!)

**qwen-turbo (7B) handles all NLP + Discovery**:
- Language detection (context-aware, mixed-language)
- Topic clustering (semantic understanding)
- Entity extraction (multilingual, zero setup)
- Keyword extraction (context-aware)
- **Cross-domain expansion** (intelligent feed discovery)
- **Related domain suggestions** (PropTech â†’ IoT, Smart Home, etc.)

**Economics**:
- Cost: $0.06/1M tokens = $0.003/month for MVP
- Speed: ~100+ tokens/sec
- Setup: Zero (no downloads)

### Intelligent Feed Discovery (Zero Manual Input!)

**Problem**: User doesn't know all competitors or related sources

**Solution**: 4-Stage Automatic Discovery

1. **Keyword â†’ SERP â†’ Domains**: Search seed keywords, extract top domains
2. **LLM Cross-Domain**: Expand to related verticals (PropTech â†’ IoT, Smart Home)
3. **feedfinder2**: Auto-detect RSS on each domain
4. **News API**: Aggregate 1000+ sources via keyword search

**Result**: 80-150 feeds discovered from just 2-3 seed keywords!

**User Input** (minimal):
```yaml
seed_keywords: ["PropTech", "Smart Building"]  # That's it!
```

**System discovers**:
- Primary competitors (via SERP)
- Cross-domain sources (via LLM)
- Niche blogs (via News API)
- Curated feeds (via awesome-rss-feeds)

### Patterns to Copy (NOT pip install)

| Pattern | Source | File | Status |
|---------|--------|------|--------|
| **Autocomplete Scraping** | seo-keyword-research-tool | `src/collectors/autocomplete_collector.py` | âœ… Phase 1 |
| **Content Scoring** | RankCraft-AI + ContentSwift | `src/utils/content_scorer.py` | ğŸ”œ Phase 2 |
| **SERP Top 10 Analysis** | RankCraft-AI | `src/collectors/serp_collector.py` | ğŸ”œ Phase 2 |
| **Content Gap Analysis** | ALwrity pattern | `src/analyzers/gap_analyzer.py` | ğŸ”œ Phase 2 |

---

## 5. Implementation Phases (âœ… Validated & Simplified)

### Phase 1: Foundation (Weeks 1-2) - Scope Reduced per Consensus

**Goals**:
- âœ… Optimized SQLite setup with monitoring
- âœ… 2-stage feed discovery (NOT 4-stage)
- âœ… Simple TF-IDF clustering (NO embeddings, NO per-language)
- âœ… Working agent for 1 domain/market/language
- âœ… LLM caching from day 1
- Notion sync for top topics

### File Structure
```
src/
â”œâ”€â”€ agents/                       # âœ… EXISTING from Content Creator
â”‚   â”œâ”€â”€ base_agent.py             # âœ… Base agent class (100% coverage)
â”‚   â”œâ”€â”€ competitor_research_agent.py  # âœ… Competitor analysis (100% coverage)
â”‚   â”œâ”€â”€ keyword_research_agent.py     # âœ… SEO keyword research (100% coverage)
â”‚   â””â”€â”€ content_pipeline.py       # ğŸ”œ NEW: 5-stage enhanced pipeline
â”œâ”€â”€ collectors/
â”‚   â”œâ”€â”€ base_collector.py         # Abstract base
â”‚   â”œâ”€â”€ rss_collector.py          # feedparser + awesome-rss-feeds
â”‚   â”œâ”€â”€ reddit_collector.py       # PRAW
â”‚   â”œâ”€â”€ trends_collector.py       # pytrends
â”‚   â”œâ”€â”€ autocomplete_collector.py # seo-keyword-research-tool pattern
â”‚   â””â”€â”€ competitor_collector.py   # advertools sitemap crawling
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ document.py               # Unified data model
â”‚   â””â”€â”€ topic.py                  # Topic model (already exists)
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ deduplicator.py           # MinHash/LSH
â”‚   â””â”€â”€ llm_processor.py          # qwen-turbo (language, clustering, entities)
â”œâ”€â”€ research/
â”‚   â””â”€â”€ deep_researcher.py        # gpt-researcher wrapper
â”œâ”€â”€ database/
â”‚   â””â”€â”€ sqlite_manager.py         # SQLite operations
â”œâ”€â”€ tasks/
â”‚   â””â”€â”€ huey_tasks.py             # Background jobs
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ models.yaml               # LLM config (already exists)
â”‚   â””â”€â”€ language_resources.yaml   # Language-specific feeds/forums
â””â”€â”€ main.py                       # Entry point
```

**Note**: Competitor and keyword research agents are already implemented from the Content Creator system with 100% test coverage and production-ready. They will be integrated into the new pipeline.

### Week 1: Foundation (âœ… Consensus Validated)
**Priority**: Critical infrastructure with mitigations built-in

1. **Central Logging** (`src/utils/logger.py`)
   - structlog setup with correlation IDs
   - Metrics tracking hooks (WAL size, API usage, error rates)

2. **Configuration System** (`src/utils/config_loader.py`)
   - Pydantic validation for market configs
   - OPML seed lists (German PropTech, French Fashion)

3. **Optimized SQLite** (`src/database/connection.py`)
   - WAL mode with aggressive checkpointing (`wal_autocheckpoint=1000`)
   - Connection pool (single writer pattern)
   - Health monitoring (WAL size alerts)

4. **Document Model** (`src/models/document.py`)
   - Unified data structure across all collectors
   - Language field (auto-detected, cached)

5. **LLM Processor** (`src/processors/llm_processor.py`)
   - qwen-turbo via OpenRouter
   - 30-day response caching (with cache key: prompt + model + version)
   - Pydantic validation (NOT Instructor library)
   - Retry logic + circuit breaker

6. **Deduplicator** (`src/processors/deduplicator.py`)
   - MinHash/LSH + canonical URL normalization
   - Persistent bloom filter for fast lookups

7. **Huey Setup** (`src/tasks/huey_tasks.py`)
   - SQLite-backed queue (single writer)
   - DLQ (dead-letter queue) for failed jobs
   - Retry logic with exponential backoff

### Week 2: Feed Discovery & Clustering (âœ… Simplified Approach)
**Priority**: 2-stage discovery + simple clustering

1. **Feed Discovery Pipeline** (`src/collectors/feed_discovery.py`)
   - Stage 1: OPML seeds + Gemini CLI expansion (with fallback)
   - Stage 2: SerpAPI (3/day hard cap) + feedfinder2
   - 30-day SERP result caching
   - Circuit breaker + graceful degradation

2. **RSS Collector** (`src/collectors/rss_collector.py`)
   - feedparser + ETag/Last-Modified (conditional GET)
   - trafilatura for content extraction (handle summary-only feeds)
   - Feed health tracking (adaptive polling, failure counts)
   - Per-host rate limiting + robots.txt respect

3. **Simple TF-IDF Clustering** (`src/processors/clustering.py`)
   - NO embeddings, NO per-language separation (per-config = single language)
   - HDBSCAN for density-based clustering (no need to guess K)
   - LLM labeling only (cheap, explainable)

4. **Deep Research Integration** (`src/research/deep_researcher.py`)
   - gpt-researcher wrapper
   - Integrate with existing CompetitorResearchAgent + KeywordResearchAgent

### Acceptance Criteria (âœ… Validated Metrics)
- [ ] Discovers 50+ unique topics/week for German PropTech test config
- [ ] Deduplication rate <5% (MinHash/LSH + canonical URLs)
- [ ] Language detection >95% accurate (qwen-turbo cached)
- [ ] SerpAPI usage â‰¤3 requests/day (circuit breaker functioning)
- [ ] SQLite WAL size <10MB (checkpoint monitoring working)
- [ ] Gemini CLI success rate >95% (fallback logic tested)
- [ ] LLM cache hit rate >60% (30-day TTL effective)
- [ ] Feed health tracking operational (adaptive polling working)
- [ ] TF-IDF clustering produces semantically meaningful topics
- [ ] Deep research generates 5-6 page reports with citations
- [ ] Runs automated (daily collection, no manual intervention)

---

## 5. Phase 2: Content Intelligence (Week 3-4)

### Commercial Tool Patterns

#### From ContentSwift/Surfer SEO
```python
class ContentScorer:
    """0-100 scoring: keyword + readability + structure + entities"""
    def score_content(content, keyword, entities): pass
```

#### From MarketMuse
```python
class ContentIntelligence:
    """Topic authority + content gaps + difficulty scoring"""
    def detect_topic_authority(): pass
    def find_content_gaps(competitors): pass
    def calculate_difficulty_score(topic): pass
```

#### From RankCraft-AI
```python
class SERPAnalyzer:
    """Top 10 SERP analysis (FREE: DuckDuckGo)"""
    def scrape_top_10(keyword): pass
    def analyze_structure(): pass  # Common headings, word counts, entities
```

### Week 3: SERP & Scoring
- ğŸ”œ SERP Top 10 analyzer (RankCraft-AI pattern)
- ğŸ”œ Content scoring algorithm (0-100 scale)
- ğŸ”œ Keyword density + variations
- ğŸ”œ Readability scoring (textstat)
- ğŸ”œ Entity coverage analysis

### Week 4: Intelligence & Gaps
- ğŸ”œ Topic authority detection (BERTopic clusters)
- ğŸ”œ Content gap analysis (competitors vs ours)
- ğŸ”œ Difficulty scoring (personalized)
- ğŸ”œ Internal linking suggestions
- ğŸ”œ Performance tracking setup

### Acceptance Criteria
- [ ] Content scores match commercial tools (Surfer SEO benchmark)
- [ ] Content gap analysis identifies 20+ missing topics
- [ ] Difficulty scores correlate with actual ranking time
- [ ] SERP analysis covers 100+ keywords

---

## 6. Phase 3: Production (Week 5-6)

### Week 5: Scalability
- ğŸ”® Postgres migration (keep SQLite for dev)
- ğŸ”® pgvector for similarity search
- ğŸ”® Huey + Redis (if distributed workers needed)
- ğŸ”® Source reliability scoring
- ğŸ”® Compliance logging (robots.txt, attribution)

### Week 6: Multi-Niche & Analytics
- ğŸ”® Test with 3+ different configs (validate universal design)
- ğŸ”® Feed manager UI (Streamlit)
- ğŸ”® Analytics dashboard (source performance)
- ğŸ”® Multi-platform publishing (WordPress, Webflow, Medium)
- ğŸ”® Google Search Console integration

### Acceptance Criteria
- [ ] Handles 3+ niches simultaneously
- [ ] Postgres supports 100K+ documents
- [ ] Analytics dashboard shows ROI per source
- [ ] Multi-platform publishing works

---

## 7. Implementation Details

### Database Schema (Universal)
```sql
-- Documents (raw collected content)
CREATE TABLE documents (
    id TEXT PRIMARY KEY,
    source TEXT NOT NULL,           -- "rss_heise", "reddit_proptech"
    source_url TEXT,

    -- Content
    title TEXT NOT NULL,
    content TEXT,
    summary TEXT,

    -- Classification
    language TEXT NOT NULL,         -- ISO 639-1 (de, en, fr)
    domain TEXT,                    -- SaaS, E-commerce
    market TEXT,                    -- Germany, France
    vertical TEXT,                  -- Proptech, Fashion

    -- Deduplication
    content_hash TEXT,              -- SimHash for near-duplicates
    canonical_url TEXT,

    -- Metadata
    published_at TIMESTAMP,
    fetched_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    author TEXT,

    -- Provenance
    reliability_score REAL DEFAULT 0.5,
    paywall BOOLEAN DEFAULT 0,

    -- Status
    status TEXT DEFAULT 'new',      -- new, processed, rejected
    raw_json TEXT                   -- Original response for debugging
);

CREATE INDEX idx_documents_hash ON documents(content_hash);
CREATE INDEX idx_documents_lang ON documents(language);
CREATE INDEX idx_documents_status ON documents(status);

-- Full-text search
CREATE VIRTUAL TABLE documents_fts USING fts5(
    title, content,
    content=documents,
    tokenize="unicode61 remove_diacritics 2"
);

-- Topics (clustered and scored)
CREATE TABLE topics (
    id TEXT PRIMARY KEY,
    title TEXT NOT NULL,
    cluster_label TEXT,

    -- Scores (0-1 scale)
    demand_score REAL,              -- Search volume, engagement
    opportunity_score REAL,         -- Competition level
    fit_score REAL,                 -- Relevance to domain/market/vertical
    novelty_score REAL,             -- How unique vs existing content
    priority_score REAL,            -- Weighted combination

    -- Classification
    language TEXT,
    domain TEXT,
    market TEXT,
    vertical TEXT,

    -- Keywords & entities
    primary_keyword TEXT,
    keywords JSON,                  -- ["DSGVO", "Immobilien"]
    entities JSON,                  -- ["Berlin", "BaFin"]

    -- Provenance
    source_documents JSON,          -- [doc_id1, doc_id2, ...]

    -- Status
    status TEXT DEFAULT 'backlog',  -- backlog, researching, approved, published
    notion_id TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_topics_priority ON topics(priority_score DESC);
CREATE INDEX idx_topics_status ON topics(status);
```

### Configuration System
```yaml
# config/proptech_dach.yaml
domain: "SaaS"
market: "DACH"
language: "de"
vertical: "Proptech"

seed_keywords:
  - "DSGVO"
  - "Immobilien SaaS"
  - "Smart Building"
  - "PropTech"

competitor_urls:
  - "https://www.immobilienscout24.de"
  - "https://www.propstack.de"

collectors:
  rss:
    enabled: true
    feeds_source: "awesome-rss-feeds"  # Auto-load from GitHub
    custom_feeds:
      - "https://www.heise.de/rss/heise-atom.xml"
      - "https://t3n.de/rss.xml"

  reddit:
    enabled: true
    auto_discover: true               # Gemini suggests subreddits
    manual_subreddits:
      - "de"
      - "Finanzen"

  trends:
    enabled: true
    region: "DE"
    lookback_days: 30

  autocomplete:
    enabled: true
    max_suggestions: 100

scheduling:
  collection_time: "02:00"            # Daily at 2 AM
  notion_sync_day: "monday"           # Weekly sync
  lookback_days: 7
```

### Code Patterns

#### Unified Document Model
```python
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, List
import numpy as np

@dataclass
class Document:
    """Universal data model for ALL sources"""
    # Identity
    id: str
    source: str                      # "rss_heise", "reddit_proptech"
    source_url: str

    # Content
    title: str
    content: str
    summary: Optional[str] = None

    # Classification
    language: str                    # ISO 639-1 (auto-detected)
    domain: str                      # From config
    market: str                      # From config
    vertical: str                    # From config

    # Deduplication
    content_hash: str                # SimHash/MinHash
    canonical_url: str

    # Metadata
    published_at: datetime
    fetched_at: datetime
    author: Optional[str] = None

    # Enrichment (added in processing)
    entities: List[str] = None
    keywords: List[str] = None
    embeddings: Optional[np.array] = None

    # Provenance
    reliability_score: float = 0.5
    paywall: bool = False

    # Status
    status: str = "new"
```

#### Deduplication
```python
from datasketch import MinHash, MinHashLSH

class Deduplicator:
    def __init__(self, threshold=0.7):
        self.lsh = MinHashLSH(threshold=threshold, num_perm=128)

    def is_duplicate(self, doc: Document) -> bool:
        """Check if document is near-duplicate (>70% similarity)"""
        minhash = MinHash(num_perm=128)
        for word in doc.content.split():
            minhash.update(word.encode('utf8'))

        duplicates = self.lsh.query(minhash)
        return len(duplicates) > 0
```

#### Huey Task Queue
```python
from huey import SqliteHuey
from datetime import datetime, timedelta

huey = SqliteHuey(filename='tasks.db')

@huey.task()
def collect_all_sources():
    """Background task: Collect from all enabled sources"""
    agent = UniversalTopicAgent.load_config('config/proptech_dach.yaml')
    agent.collect_all_sources()

@huey.periodic_task(crontab(hour=2, minute=0))
def daily_collection():
    """Scheduled task: Daily at 2 AM"""
    collect_all_sources()

@huey.periodic_task(crontab(day_of_week='1', hour=9))
def weekly_notion_sync():
    """Scheduled task: Monday at 9 AM"""
    agent = UniversalTopicAgent.load_config('config/proptech_dach.yaml')
    agent.sync_to_notion()
```

#### LLM Processor (Replaces 5GB NLP Stack!)
```python
from openai import OpenAI
import json
import os

class LLMProcessor:
    """
    Single qwen-turbo (7B) replaces:
    - fasttext (1GB) - language detection
    - BERTopic (500MB + 2GB) - topic clustering
    - spaCy (500MB/lang) - entity extraction

    Cost: $0.06/1M tokens = $0.003/month for MVP
    Speed: ~100+ tokens/sec = 0.5s per call
    """

    def __init__(self):
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY")
        )
        self.model = "qwen/qwen-2.5-7b-instruct"  # Fast + cheap

    def detect_language(self, text: str) -> dict:
        """
        Replace fasttext (1GB model)
        Better: Context-aware, handles mixed-language
        """
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "user",
                "content": f"""Detect language. Return JSON only.

Text: {text[:500]}

Format: {{"language": "de|en|fr", "confidence": 0-1}}"""
            }],
            temperature=0,
            max_tokens=30
        )
        return json.loads(response.choices[0].message.content)

    def cluster_topics(self, topics: list[str]) -> dict:
        """Replace BERTopic (500MB + 2GB models)"""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "user",
                "content": f"""Group {len(topics)} topics into 5-10 clusters:

{json.dumps(topics[:50], ensure_ascii=False)}

JSON: [{{"cluster": "name", "topics": [...]}}]"""
            }],
            temperature=0.3,
            max_tokens=2000
        )
        return json.loads(response.choices[0].message.content)

    def extract_entities_keywords(self, content: str, language: str) -> dict:
        """Replace spaCy NER (500MB per language)"""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "user",
                "content": f"""Extract from {language} content:
1. Named entities (companies, products, people, places)
2. Top 10 keywords

Content: {content[:1500]}

JSON: {{"entities": [...], "keywords": [...]}}"""
            }],
            temperature=0,
            max_tokens=300
        )
        return json.loads(response.choices[0].message.content)
```

#### Deep Research Wrapper
```python
from gpt_researcher import GPTResearcher

class DeepResearcher:
    def __init__(self):
        self.researcher = GPTResearcher(
            llm_provider="google_genai",
            smart_llm_model="gemini-2.0-flash-exp",
            fast_llm_model="gemini-2.0-flash-exp",
            retriever="duckduckgo",    # FREE
            max_sources=8,
            report_format="markdown"
        )

    async def research_topic(self, topic: str, config: dict):
        """Deep research with context"""
        contextualized_query = (
            f"{topic} "
            f"in {config['domain']} industry "
            f"for {config['market']} market "
            f"in {config['language']} language "
            f"focused on {config['vertical']}"
        )

        report = await self.researcher.conduct_research(contextualized_query)

        return {
            'topic': topic,
            'report': report.report,      # Full markdown
            'citations': report.sources,  # [1], [2], ...
            'created_at': datetime.now()
        }
```

#### Competitor Research Agent (Integrated from Content Creator)
```python
from src.agents.competitor_research_agent import CompetitorResearchAgent

class CompetitorResearcher:
    """
    Analyze competitors to identify content gaps and opportunities

    Uses Gemini CLI (FREE) for web search + social analysis
    Caches results for 7 days
    """

    def __init__(self, api_key: str):
        self.agent = CompetitorResearchAgent(api_key=api_key)

    async def research_competitors(
        self,
        topic: str,
        language: str = "de",
        max_competitors: int = 5
    ) -> dict:
        """
        Returns:
        {
            "competitors": [
                {
                    "name": str,
                    "website": str,
                    "description": str,
                    "social_handles": {"linkedin": str, ...},
                    "content_strategy": {
                        "topics": List[str],
                        "posting_frequency": str,
                        "content_types": List[str],
                        "strengths": List[str],
                        "weaknesses": List[str]
                    }
                }
            ],
            "content_gaps": List[str],      # Opportunities not covered
            "trending_topics": List[str],   # Popular in niche
            "recommendation": str           # Strategic advice
        }
        """
        return await self.agent.research_competitors(
            topic=topic,
            language=language,
            max_competitors=max_competitors,
            include_content_analysis=True
        )
```

#### Keyword Research Agent (Integrated from Content Creator)
```python
from src.agents.keyword_research_agent import KeywordResearchAgent

class KeywordResearcher:
    """
    SEO keyword research for content optimization

    Uses Gemini CLI (FREE) for keyword data + Google Trends
    Caches results for 30 days
    """

    def __init__(self, api_key: str):
        self.agent = KeywordResearchAgent(api_key=api_key)

    async def research_keywords(
        self,
        topic: str,
        language: str = "de",
        target_audience: str = "",
        keyword_count: int = 10
    ) -> dict:
        """
        Returns:
        {
            "primary_keyword": {
                "keyword": str,
                "search_volume": str,       # "1K-10K"
                "competition": str,         # "Low|Medium|High"
                "difficulty": int,          # 0-100
                "intent": str              # "Informational|Commercial"
            },
            "secondary_keywords": [...],    # Similar structure
            "long_tail_keywords": [...],    # 3-5 word phrases
            "related_questions": List[str], # "People also ask"
            "search_trends": {
                "trending_up": List[str],
                "trending_down": List[str],
                "seasonal": bool
            },
            "recommendation": str
        }
        """
        return await self.agent.research_keywords(
            topic=topic,
            language=language,
            target_audience=target_audience,
            keyword_count=keyword_count
        )
```

#### Enhanced Content Pipeline (5-Stage with Research)
```python
async def enhanced_content_pipeline(topic: str, config: dict):
    """
    Complete pipeline with competitor & keyword research

    Stages:
    1. Competitor Analysis - Identify gaps and opportunities
    2. Keyword Research - Find high-value keywords
    3. Deep Research - Generate sourced content (gpt-researcher)
    4. Content Optimization - Apply SEO insights
    5. Notion Sync - Store with metadata
    """

    # Stage 1: Competitor Analysis
    competitor_researcher = CompetitorResearcher(api_key=config['api_key'])
    competitor_data = await competitor_researcher.research_competitors(
        topic=topic,
        language=config['language'],
        max_competitors=5
    )

    logger.info("competitor_analysis_complete",
                competitors_found=len(competitor_data['competitors']),
                content_gaps=len(competitor_data['content_gaps']))

    # Stage 2: Keyword Research
    keyword_researcher = KeywordResearcher(api_key=config['api_key'])
    keyword_data = await keyword_researcher.research_keywords(
        topic=topic,
        language=config['language'],
        target_audience=config.get('target_audience', ''),
        keyword_count=10
    )

    logger.info("keyword_research_complete",
                primary_keyword=keyword_data['primary_keyword']['keyword'],
                secondary_count=len(keyword_data['secondary_keywords']),
                long_tail_count=len(keyword_data['long_tail_keywords']))

    # Stage 3: Deep Research (enhanced with insights)
    deep_researcher = DeepResearcher()

    # Enhance query with competitor gaps and keywords
    enhanced_query = (
        f"{topic} "
        f"focusing on: {', '.join(competitor_data['content_gaps'][:3])} "
        f"targeting keyword: {keyword_data['primary_keyword']['keyword']} "
        f"in {config['domain']} for {config['market']} market"
    )

    research_report = await deep_researcher.research_topic(
        enhanced_query,
        config
    )

    logger.info("deep_research_complete",
                report_length=len(research_report['report']),
                citations=len(research_report['citations']))

    # Stage 4: Content Optimization
    optimized_content = {
        'topic': topic,
        'report': research_report['report'],
        'citations': research_report['citations'],

        # SEO metadata
        'primary_keyword': keyword_data['primary_keyword'],
        'secondary_keywords': keyword_data['secondary_keywords'],
        'long_tail_keywords': keyword_data['long_tail_keywords'],
        'related_questions': keyword_data['related_questions'],

        # Competitive intelligence
        'competitors': competitor_data['competitors'],
        'content_gaps': competitor_data['content_gaps'],
        'differentiation_strategy': competitor_data['recommendation'],

        # Metadata
        'created_at': datetime.now(),
        'language': config['language'],
        'domain': config['domain'],
        'market': config['market']
    }

    # Stage 5: Store in database
    db.save_research_report(optimized_content)

    logger.info("content_pipeline_complete",
                topic=topic,
                competitor_insights=True,
                keyword_optimized=True)

    return optimized_content
```

**Integration Points**:
- Competitor research runs BEFORE deep research to identify gaps
- Keyword research informs content structure and headings
- Both results stored in Notion for editorial review
- Content gaps become new topic suggestions
- Trending topics from competitors feed back into discovery

**Caching Strategy**:
- Competitors: 7 days TTL (strategies change slowly)
- Keywords: 30 days TTL (search data relatively stable)
- Research reports: Permanent (timestamped for tracking)

**File Structure Addition**:
```
src/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ competitor_research_agent.py   # âœ… Already implemented
â”‚   â”œâ”€â”€ keyword_research_agent.py      # âœ… Already implemented
â”‚   â””â”€â”€ content_pipeline.py            # ğŸ”œ New: 5-stage pipeline
```

---

## 8. Appendix: Key Patterns

### Intelligent Feed Discovery (Zero Manual Input)
```python
from duckduckgo_search import DDGS
import feedfinder2
from urllib.parse import urlparse

class IntelligentFeedDiscovery:
    """
    Automatic feed discovery WITHOUT knowing competitors

    User provides: 2-3 seed keywords
    System discovers: 80-150 feeds automatically
    """

    def __init__(self, llm_processor: LLMProcessor):
        self.llm = llm_processor

    async def discover_all(self, config: MarketConfig) -> List[str]:
        """
        4-Stage intelligent discovery
        """
        all_feeds = set()

        # STAGE 1: Keyword â†’ SERP â†’ Domains â†’ Feeds
        logger.info("Stage 1: SERP-based discovery")
        serp_feeds = await self._discover_from_serp(config.seed_keywords)
        all_feeds.update(serp_feeds)
        logger.info(f"  Found {len(serp_feeds)} feeds from SERP")

        # STAGE 2: LLM cross-domain expansion
        logger.info("Stage 2: Cross-domain expansion")
        related_domains = await self._expand_domains(config.domain, config.vertical)

        for domain in related_domains[:5]:  # Top 5 related domains
            domain_keywords = [f"{domain} {config.market}"]
            domain_feeds = await self._discover_from_serp(domain_keywords)
            all_feeds.update(domain_feeds[:5])  # Top 5 per domain

        logger.info(f"  Found {len(all_feeds) - len(serp_feeds)} cross-domain feeds")

        # STAGE 3: Curated feeds (awesome-rss-feeds)
        logger.info("Stage 3: Curated baseline")
        curated = self._load_curated_feeds(config.market)
        all_feeds.update(curated[:20])

        # STAGE 4: News API (optional)
        if config.news_api_enabled:
            logger.info("Stage 4: News API aggregation")
            news_feeds = await self._discover_from_news_api(config.seed_keywords)
            all_feeds.update(news_feeds)

        # Validate feeds
        valid_feeds = await self._validate_feeds(list(all_feeds))

        logger.info(f"Total: {len(valid_feeds)} valid feeds discovered")
        return valid_feeds

    async def _discover_from_serp(self, keywords: List[str]) -> List[str]:
        """
        Search keywords â†’ Extract domains â†’ Find feeds
        """
        domains = set()

        with DDGS() as ddgs:
            for keyword in keywords:
                results = list(ddgs.text(keyword, max_results=20))

                for result in results:
                    domain = self._extract_domain(result['href'])
                    if domain:
                        domains.add(domain)

        # Find RSS feeds on each domain
        feeds = []
        for domain in domains:
            try:
                domain_feeds = feedfinder2.find_feeds(f"https://{domain}")
                feeds.extend(domain_feeds)
            except Exception as e:
                logger.debug(f"Could not find feeds on {domain}: {e}")
                continue

        return feeds

    async def _expand_domains(self, domain: str, vertical: str) -> List[str]:
        """
        LLM suggests related domains/verticals

        PropTech â†’ [real-estate, IoT, smart-home, construction-tech, ...]
        """
        response = self.llm.client.chat.completions.create(
            model=self.llm.model,
            messages=[{
                "role": "user",
                "content": f"""List 10 related domains/verticals for content discovery:

Primary: {domain} / {vertical}

Include cross-domain topics that are relevant but not obvious.

Return JSON array: ["domain1", "domain2", ...]

Example for PropTech:
- real-estate (core)
- IoT (sensors)
- smart-home (automation)
- construction-tech (building)
- energy-management (sustainability)
- facility-management (operations)"""
            }],
            temperature=0.3,
            max_tokens=200
        )

        related = json.loads(response.choices[0].message.content)
        logger.info(f"LLM suggested {len(related)} related domains: {related[:3]}...")
        return related

    def _extract_domain(self, url: str) -> str:
        """Extract domain from URL"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc or parsed.path
            # Remove www.
            domain = domain.replace('www.', '')
            return domain
        except:
            return None

    def _load_curated_feeds(self, market: str) -> List[str]:
        """
        Load curated feeds from awesome-rss-feeds
        """
        # Simplified - real implementation would parse OPML
        curated_map = {
            'de': [
                'https://www.heise.de/rss/heise-atom.xml',
                'https://t3n.de/rss.xml',
                'https://www.golem.de/rss.php'
            ],
            'us': [
                'https://techcrunch.com/feed/',
                'https://www.theverge.com/rss/index.xml'
            ]
        }
        return curated_map.get(market, [])

    async def _validate_feeds(self, feeds: List[str]) -> List[str]:
        """
        Validate that feeds are accessible and contain RSS/Atom
        """
        valid = []
        async with httpx.AsyncClient(timeout=10.0) as client:
            for feed_url in feeds:
                try:
                    response = await client.get(feed_url)
                    if response.status_code == 200:
                        content = response.text[:500].lower()
                        if any(tag in content for tag in ['<rss', '<feed', '<atom']):
                            valid.append(feed_url)
                except Exception:
                    continue

        return valid
```

**Usage**:
```python
# Minimal config:
config = MarketConfig(
    domain="proptech",
    market="de",
    seed_keywords=["PropTech", "Smart Building"]  # Just 2 keywords!
)

# Automatic discovery:
discovery = IntelligentFeedDiscovery(llm_processor)
feeds = await discovery.discover_all(config)

# Result: 80-150 feeds without knowing competitors! ğŸ‰
```

### ContentSwift Pattern (Content Scoring)
```python
# From RankCraft-AI + ContentSwift
def score_content(content: str, keyword: str, entities: List[str]) -> Dict:
    """0-100 score: keyword (25) + readability (25) + structure (25) + entities (25)"""
    score = {
        'keyword_optimization': score_keyword_density(content, keyword),       # 0-25
        'readability': textstat.flesch_reading_ease(content) / 4,              # 0-25
        'structure': score_structure(content),                                  # 0-25
        'entity_coverage': score_entity_coverage(content, entities),           # 0-25
        'total': 0
    }
    score['total'] = sum([score['keyword_optimization'], score['readability'],
                          score['structure'], score['entity_coverage']])
    return score
```

### ALwrity Pattern (Gap Analysis)
```python
def find_content_gaps(competitor_urls: List[str], our_topics: List[str]) -> List[str]:
    """Compare competitor topics vs our content"""
    competitor_topics = set()
    for url in competitor_urls:
        sitemap_df = advertools.sitemap_to_df(f"{url}/sitemap.xml")
        topics = extract_topics_from_sitemap(sitemap_df)
        competitor_topics.update(topics)

    gaps = competitor_topics - set(our_topics)
    return sorted(gaps, key=lambda t: t['demand_score'], reverse=True)
```

### RankCraft-AI Pattern (SERP Analysis)
```python
from duckduckgo_search import DDGS

def scrape_top_10(keyword: str) -> List[Dict]:
    """FREE SERP scraping (DuckDuckGo)"""
    with DDGS() as ddgs:
        results = list(ddgs.text(keyword, max_results=10))

    return [
        {'title': r['title'], 'url': r['href'], 'snippet': r['body'], 'rank': i+1}
        for i, r in enumerate(results)
    ]

def analyze_serp_structure(results: List[Dict]) -> Dict:
    """Extract common patterns from top 10"""
    analysis = {'avg_word_count': 0, 'common_headings': [], 'common_entities': []}

    for result in results:
        content = fetch_and_extract(result['url'])
        analysis['word_counts'].append(len(content.split()))
        analysis['headings'].extend(extract_headings(content))

    analysis['avg_word_count'] = sum(analysis['word_counts']) / len(analysis['word_counts'])
    analysis['common_headings'] = find_frequent(analysis['headings'], min_count=3)

    return analysis
```

### AnswerThePublic Pattern (Autocomplete)
```python
def extract_autocomplete_data(seed_keyword: str, language: str = "en") -> List[str]:
    """FREE autocomplete scraping"""
    suggestions = []

    # Alphabet pattern
    for letter in 'abcdefghijklmnopqrstuvwxyz':
        url = "https://suggestqueries.google.com/complete/search"
        params = {'q': f"{seed_keyword} {letter}", 'client': 'firefox', 'hl': language}
        response = httpx.get(url, params=params)
        suggestions.extend(response.json()[1])

    # Question patterns
    for prefix in ['what', 'how', 'why', 'when', 'where', 'who']:
        params = {'q': f"{prefix} {seed_keyword}", 'client': 'firefox', 'hl': language}
        response = httpx.get(url, params=params)
        suggestions.extend(response.json()[1])

    return list(set(suggestions))
```

### main.py (Dependency Injection Example)
```python
# src/main.py
import asyncio
from src.utils.logger import setup_logging, get_logger
from src.utils.config_loader import ConfigLoader
from src.database.sqlite_manager import DatabaseManager
from src.processors.llm_processor import LLMProcessor
from src.processors.deduplicator import Deduplicator
from src.collectors.rss_collector import RSSCollector
from src.collectors.reddit_collector import RedditCollector
from src.research.deep_researcher import DeepResearcher

logger = get_logger(__name__)

async def main():
    """
    Entry point with dependency injection

    No global state - all dependencies explicit!
    """
    # 1. Setup (once)
    setup_logging(log_level="INFO")
    logger.info("application_started")

    # 2. Load config
    config = ConfigLoader.load("proptech_de")
    logger.info("config_loaded",
               domain=config.market.domain,
               market=config.market.market)

    # 3. Initialize shared dependencies
    db = DatabaseManager(config.database)
    llm = LLMProcessor()  # For language detection, clustering, etc.
    dedup = Deduplicator(threshold=0.7)

    logger.info("dependencies_initialized")

    # 4. Initialize collectors (with dependency injection)
    collectors = [
        RSSCollector(
            config=config.market,
            db=db,
            llm=llm,
            dedup=dedup
        ),
        RedditCollector(
            config=config.market,
            db=db,
            llm=llm,
            dedup=dedup
        ),
        # ... more collectors
    ]

    logger.info("collectors_initialized", count=len(collectors))

    # 5. Run collection
    all_documents = []
    for collector in collectors:
        try:
            documents = await collector.collect()
            all_documents.extend(documents)
            logger.info("collector_success",
                       collector=collector.__class__.__name__,
                       doc_count=len(documents))
        except Exception as e:
            logger.error("collector_failed",
                        collector=collector.__class__.__name__,
                        error=str(e))

    logger.info("collection_complete", total_documents=len(all_documents))

    # 6. Process & cluster topics
    unique_docs = await dedup.filter_duplicates(all_documents)
    topics = await llm.cluster_topics([d.title for d in unique_docs])

    logger.info("processing_complete",
               unique_docs=len(unique_docs),
               topics=len(topics))

    # 7. Deep research on top topics
    researcher = DeepResearcher(llm=llm)
    for topic in topics[:10]:  # Top 10
        report = await researcher.research_topic(topic, config.market)
        db.save_research_report(report)

        logger.info("research_complete",
                   topic=topic,
                   citations=len(report['citations']))

    logger.info("application_complete")

if __name__ == "__main__":
    asyncio.run(main())
```

**Key principles demonstrated**:
1. âœ… **No global state** - Everything passed explicitly
2. âœ… **Central logging** - `setup_logging()` called once
3. âœ… **No import circles** - Dependencies flow down
4. âœ… **Testable** - Easy to mock db, llm, etc.
5. âœ… **Clear composition** - main.py wires everything together

---

## Decision Status

| Decision | Status | Notes |
|----------|--------|-------|
| **Task Queue: Huey** | âœ… FINAL | Simple, no Redis needed |
| **Database: SQLite â†’ Postgres** | âœ… FINAL | MVP local, prod scalable |
| **Deep Research: gpt-researcher** | âœ… FINAL | pip install, citations built-in |
| **RSS-First Strategy** | âœ… FINAL | 500+ feeds, unlimited |
| **Article Extraction: trafilatura** | âœ… FINAL | Primary, newspaper3k fallback |
| **Feed Discovery: feedfinder2 + RSSHub** | âœ… FINAL | Auto-detect + generate |
| **Job Scheduling: APScheduler â†’ Huey** | âœ… FINAL | Evolution path defined |
| **Observability: structlog** | âœ… FINAL | Structured logging |
| **NLP Stack: qwen-turbo (7B)** | âœ… FINAL | Replaces 5GB dependencies |
| **Content Scoring** | âš ï¸ PHASE 2 | RankCraft-AI pattern |
| **SERP Analysis** | âš ï¸ PHASE 2 | DuckDuckGo (FREE) |
| **Multi-platform Publishing** | âš ï¸ PHASE 3 | WordPress, Webflow, Medium |

---

## Success Metrics

### After 1 Month (MVP)
- [ ] 100+ topics discovered across 3 test configs
- [ ] Deduplication: <5% duplicate rate
- [ ] Language accuracy: >95% correct
- [ ] Processing: <60 seconds full pipeline
- [ ] Notion sync: 100% success rate
- [ ] Database: <100MB

### After 3 Months (Production)
- [ ] 500+ topics across 5+ configs
- [ ] Topic generation: <10 seconds (cached)
- [ ] Entity extraction: >90% accurate
- [ ] Source reliability: Automated scoring
- [ ] Zero manual topic input

---

## Total Lines: 580

**Ready to implement**: Phase 1 starts with `src/collectors/rss_collector.py`
